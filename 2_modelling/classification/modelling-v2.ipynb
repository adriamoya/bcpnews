{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/moyandreu/coding/bcpnews/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: iso-8859-15 -*-\n",
    "\n",
    "#%matplotlib inline\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)\n",
    "\n",
    "import re\n",
    "import pickle\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import random as rn\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# Force TensorFlow to use single thread.\n",
    "# Multiple threads are a potential source of\n",
    "# non-reproducible results.\n",
    "# For further details, see: https://stackoverflow.com/questions/42022950/which-seeds-have-to-be-set-where-to-realize-100-reproducibility-of-training-res\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "\n",
    "from keras import backend as K\n",
    "# The below tf.set_random_seed() will make random number generation\n",
    "# in the TensorFlow backend have a well-defined initial state.\n",
    "# For further details, see: https://www.tensorflow.org/api_docs/python/tf/set_random_seed\n",
    "\n",
    "# The below is necessary for starting core Python generated random numbers\n",
    "# in a well-defined state.\n",
    "# rn.seed(12345)\n",
    "\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)\n",
    "\n",
    "# Rest of code follows ...\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import (Activation, Bidirectional, Dropout, TimeDistributed,\n",
    "                          Flatten, Dense, BatchNormalization, LSTM, Embedding,\n",
    "                          Reshape, Conv1D, MaxPooling1D, AveragePooling1D, GlobalMaxPooling1D)\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import warnings\n",
    "pd.options.mode.chained_assignment = None\n",
    "warnings.filterwarnings(module='sklearn*', action='ignore', category=DeprecationWarning)\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables\n",
    "flag = 'flag'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df = pd.read_csv(\"../../1_construction/3_newspaper_scraper/analyses/cleaned_datasets/train.csv\")\n",
    "df_test  = pd.read_csv(\"../../1_construction/3_newspaper_scraper/analyses/cleaned_datasets/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ip calculation\n",
    "def ip(y_target, y_pred):\n",
    "    return 100*(2*(metrics.roc_auc_score(y_target, y_pred))-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df, column=\"text\"):\n",
    "    \"\"\"Preprocessing (lower case, remove urls, punctuations).\n",
    "    \n",
    "    Args:\n",
    "        df     : Dataset with articles information (pandas.DataFrame).\n",
    "        column : Name of the column that contains the text of the article. Default is `text`.\n",
    "        \n",
    "    Returns:\n",
    "        df     : Dataset with articles information (pandas.DataFrame).\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\nPreprocessing %s ...\" % (column))\n",
    "\n",
    "    # preprocessing steps: lower case, remove urls, punctuations ...\n",
    "    df[column] = df[column].str.lower()\n",
    "    df[column] = df[column].str.replace(r'http[\\w:/\\.]+','') # remove urls\n",
    "    df[column] = df[column].str.replace(r'[^\\.(a-zA-ZÀ-ÿ0-9)\\s]','') #remove everything but characters and punctuation ( [^\\.\\w\\s] )\n",
    "    df[column] = df[column].str.replace(r'(?<=\\d)(\\.)(?=\\d)','') #remove dots in thousands (careful with decimals!)\n",
    "    df[column] = df[column].str.replace(r'\\.\\.+','.') #replace multple periods with a single one\n",
    "    df[column] = df[column].str.replace(r'\\.',' .') #replace multple periods with a single one\n",
    "    df[column] = df[column].str.replace(r'\\(',' ') # replace brackets with white spaces\n",
    "    df[column] = df[column].str.replace(r'\\)',' ') # replace brackets with white spaces\n",
    "    df[column] = df[column].str.replace(r'\\s\\s+',' ') #replace multple white space with a single one\n",
    "    df[column] = df[column].str.strip()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dictionary(df, min_count_word=5):\n",
    "    \"\"\"Build dictionary and relationships between words and integers.\n",
    "    \n",
    "    Args:\n",
    "        df             : Dataset with articles information (pandas.DataFrame).\n",
    "        min_count_word : Only consider words that have been used more than n times. Default is 5.\n",
    "        \n",
    "    Returns:\n",
    "        word2num       : Dictionary (words to numbers).\n",
    "        num2word       : Dictionary (numbers to words).\n",
    "        n_u_words      : Length of the dictionary (number of unique words).\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\nBuilding dictionary ...\" )\n",
    "\n",
    "    # get all unique words (only consider words that have been used more than 5 times)\n",
    "    all_text = ' '.join(df.text.values)\n",
    "    words = all_text.split()\n",
    "    u_words = Counter(words).most_common()\n",
    "    u_words = [word[0] for word in u_words if word[1]>min_count_word] # we will only consider words that have been used more than 5 times\n",
    "\n",
    "    print('The number of unique words is:', \"{:,}\".format(len(u_words)))\n",
    "\n",
    "    # create the dictionary\n",
    "    word2num = dict(zip(u_words,range(len(u_words))))\n",
    "    word2num['<Other>'] = len(u_words)\n",
    "    num2word = dict(zip(word2num.values(), word2num.keys()))\n",
    "\n",
    "    num2word[len(word2num)] = '<PAD>'\n",
    "    word2num['<PAD>'] = len(word2num)\n",
    "    \n",
    "    n_u_words = len(u_words)\n",
    "\n",
    "    return word2num, num2word, n_u_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2int(df, n_u_words, column='text', word_threshold=500):\n",
    "    \"\"\"Convert words to integers and prepad sentences\n",
    "    \n",
    "    Args:\n",
    "        df             : Dataset with articles information (pandas.DataFrame)\n",
    "        n_u_words      : Length of the dictionary (number of unique words).\n",
    "        column         : Name of the column that contains the text of the article. Default is `text`.\n",
    "        word_threshold : Number of words to consider for each text (padding). Default is 500.\n",
    "        \n",
    "    Returns:\n",
    "        int_text       : Array with texts translated to integers.\n",
    "        \"\"\"\n",
    "\n",
    "    print(\"\\nConverting words to integers and prepadding ...\" )\n",
    "\n",
    "    int_text = [[word2num[word] if word in word2num else n_u_words for word in Text.split()] for Text in df[column].values] # Text.split() python2\n",
    "\n",
    "    print('The number of texts greater than %s in length is: ' % str(word_threshold), \"{:,}\".format(np.sum(np.array([len(t)>word_threshold for t in int_text]))))\n",
    "    print('The number of texts less than 50 in length is: ', \"{:,}\".format(np.sum(np.array([len(t)<50 for t in int_text]))))\n",
    "\n",
    "    for i, t in enumerate(int_text):\n",
    "        if len(t)<word_threshold:\n",
    "            int_text[i] = [word2num['<PAD>']]*(word_threshold-len(t)) + t\n",
    "        elif len(t)>word_threshold:\n",
    "            int_text[i] = t[:word_threshold]\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    return int_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test(model, X_test, column):\n",
    "    \"\"\"Make predictions in test dataset.\n",
    "    \n",
    "    Args:\n",
    "        model     : Model trained.\n",
    "        X_test    : Array with test features.\n",
    "        column    : Name of the column that contains desired feature.\n",
    "        \n",
    "    Returns:\n",
    "        pred_test : Array with test predictions.\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    # words to numbers\n",
    "    int_text = word2int(X_test, n_u_words, column, word_threshold)\n",
    "\n",
    "    X = np.array(int_text)\n",
    "\n",
    "    pred = model.predict(X)\n",
    "\n",
    "    l_pred = []\n",
    "    for item in pred:\n",
    "        l_pred.append(item[0])\n",
    "        \n",
    "    return l_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessing text ...\n",
      "\n",
      "Preprocessing text ...\n",
      "\n",
      "Preprocessing title ...\n",
      "\n",
      "Preprocessing title ...\n"
     ]
    }
   ],
   "source": [
    "# preprocessing steps: lower case, remove urls, punctuations ...\n",
    "\n",
    "# text\n",
    "df = preprocessing(df, 'text')\n",
    "df_test = preprocessing(df_test, 'text')\n",
    "\n",
    "# title\n",
    "df = preprocessing(df, 'title')\n",
    "df_test = preprocessing(df_test, 'title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building dictionary ...\n",
      "The number of unique words is: 41,112\n"
     ]
    }
   ],
   "source": [
    "# build dictionary\n",
    "min_count_word = 4\n",
    "word2num, num2word, n_u_words = build_dictionary(df, min_count_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train / Validation split ...\n",
      "X_train: (13173, 6)\n",
      "X_valid: (1464, 6)\n",
      "y_train: (13173,)\n",
      "y_valid: (1464,)\n"
     ]
    }
   ],
   "source": [
    "# train / validation split\n",
    "print(\"\\nTrain / Validation split ...\")\n",
    "\n",
    "X, y = df[df.columns[~df.columns.str.contains(flag)]].values, df[flag].values\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1, stratify=y, shuffle=True, random_state=42)\n",
    "\n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"X_valid:\", X_valid.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"y_valid:\", y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = df.columns[~df.columns.str.contains(flag)].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(X_train, columns=columns); df_train[flag] = y_train\n",
    "df_valid = pd.DataFrame(X_valid, columns=columns); df_valid[flag] = y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (13173, 7)\n",
      "Valid: (1464, 7)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train:\", df_train.shape)\n",
    "print(\"Valid:\", df_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "def prepare_text_data(train_input, test_input, max_words, max_len):\n",
    "    print('Tokenizing and padding data...')\n",
    "    tok = Tokenizer(num_words=max_words)\n",
    "    tok.fit_on_texts(train_input)\n",
    "    sequences_train = tok.texts_to_sequences(train_input)\n",
    "    sequences_test = tok.texts_to_sequences(test_input)\n",
    "\n",
    "    print('Pad sequences (samples x time)')\n",
    "    train_input_f = sequence.pad_sequences(sequences_train, maxlen=max_len)\n",
    "    test_input_f = sequence.pad_sequences(sequences_test, maxlen=max_len)\n",
    "    return train_input_f, test_input_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing and padding data...\n",
      "Pad sequences (samples x time)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Model 1 of 4. CNN architecture using feature 'title'\n",
      "--------------------------------------------------------------------------------\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 20, 100)           1000000   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 20, 100)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 18, 250)           75250     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 18, 250)           62750     \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 16, 250)           187750    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 250)               62750     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 251       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 1,388,751\n",
      "Trainable params: 1,388,751\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "fold 1: [52.7553]\n",
      "fold 2: [51.5613]\n",
      "fold 3: [51.1027]\n",
      "fold 4: [48.5281]\n",
      "\n",
      "Averaging scores in out of fold valid dataset...\n",
      "valid: [51.9306]\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Model 2 of 4. CNNLSTM architecture using feature 'title'\n",
      "--------------------------------------------------------------------------------\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, None, 100)         4111400   \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, None, 128)         64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, None, 128)         82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               91600     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 4,349,277\n",
      "Trainable params: 4,349,277\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "fold 1: [94.4088]\n",
      "fold 2: [93.9126]\n",
      "fold 3: [93.7899]\n",
      "fold 4: [93.4108]\n",
      "\n",
      "Averaging scores in out of fold valid dataset...\n",
      "valid: [94.5575]\n",
      "Tokenizing and padding data...\n",
      "Pad sequences (samples x time)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Model 3 of 4. CNN architecture using feature 'summary'\n",
      "--------------------------------------------------------------------------------\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 250, 100)          4000000   \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 250, 100)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 248, 250)          75250     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 248, 250)          62750     \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 246, 250)          187750    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_5 (Glob (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 250)               62750     \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 251       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 4,388,751\n",
      "Trainable params: 4,388,751\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "fold 1: [50.7102]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "NFOLDS = 4\n",
    "\n",
    "features = [{\"name\": \"title\", \"max_len\": 20, \"max_features\": 10000},\n",
    "           {\"name\": \"summary\", \"max_len\": 250, \"max_features\": 40000}]  # , {\"feature\": \"summary\", \"word_threshold\": 250}\n",
    "models = ['CNN', 'CNNLSTM']\n",
    "\n",
    "params = {\n",
    "    'loss_func': 'binary_crossentropy', # binary_crossentropy\n",
    "    'optimizer': 'adam', # adam, rmsprop\n",
    "    'metrics': ['accuracy'],\n",
    "    'embedding_size': 100,\n",
    "    'batch_size': 128,\n",
    "    'epochs': 3,\n",
    "    'filters': 250, # 128\n",
    "    'kernel_size': 3,\n",
    "    'hidden_dims': 250\n",
    "}\n",
    "\n",
    "train_level_2 = np.zeros((df_train.shape[0], len(models) * len(features)))\n",
    "valid_level_2 = np.zeros((df_valid.shape[0], len(models) * len(features)))\n",
    "\n",
    "for feature_counter, feature in enumerate(features):\n",
    "    \n",
    "    params['max_len'] = feature['max_len']\n",
    "    params['max_features'] = feature['max_features']\n",
    "\n",
    "    # word to integer\n",
    "    X_train, X_valid =  prepare_text_data(df_train[feature['name']].values,\n",
    "                                          df_valid[feature['name']].values,\n",
    "                                          params['max_features'],\n",
    "                                          params['max_len'])\n",
    "\n",
    "    ntrain = X_train.shape[0]\n",
    "    nvalid = X_valid.shape[0]\n",
    "    \n",
    "    for model_counter, model in enumerate(models):\n",
    "        \n",
    "        idx = feature_counter*len(features) + model_counter\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"-\"*80)\n",
    "        print(\"Model {} of {}. {} architecture using feature '{}'\".format(idx+1, len(models)*len(features), model, feature['name']))\n",
    "        print(\"-\"*80)\n",
    "\n",
    "        oof_train = np.zeros((ntrain,))\n",
    "        oof_valid = np.zeros((nvalid,))\n",
    "        oof_valid_skf = np.empty((NFOLDS, nvalid))\n",
    "\n",
    "        kf = StratifiedKFold(n_splits=NFOLDS, shuffle=False, random_state=0)\n",
    "\n",
    "        for fold_counter, (tr_index, te_index) in enumerate(kf.split(X_train, y_train)):\n",
    "\n",
    "            # Split data and target\n",
    "            X_tr = X_train[tr_index]\n",
    "            y_tr = y_train[tr_index]\n",
    "            X_te = X_train[te_index]\n",
    "            y_te = y_train[te_index]\n",
    "\n",
    "            if model == \"CNN\":\n",
    "                oof_train, oof_valid_skf = model_CNN(X_tr, X_te, y_tr, y_te, te_index, oof_train, oof_valid_skf, fold_counter, params)\n",
    "            elif model == \"LSTM\":\n",
    "                oof_train, oof_valid_skf = model_LSTM(X_tr, X_te, y_tr, y_te, te_index, oof_train, oof_valid_skf, fold_counter, params)\n",
    "            elif model == \"CNNLSTM\":\n",
    "                oof_train, oof_valid_skf = model_CNNLSTM(X_tr, X_te, y_tr, y_te, te_index, oof_train, oof_valid_skf, fold_counter, params)                \n",
    "\n",
    "        train_level_2[:, idx] = oof_train[:]\n",
    "        \n",
    "        print(\"\\nAveraging scores in out of fold valid dataset...\")\n",
    "        oof_valid[:] = oof_valid_skf.mean(axis=0)\n",
    "        valid_level_2[:, idx] = oof_valid[:]\n",
    "        score = 100*metrics.roc_auc_score(y_valid, oof_valid[:])\n",
    "        print('valid: [%.4f]' % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_CNN(X_tr, X_te, y_tr, y_te, te_index, oof_train, oof_valid_skf, fold_counter, params):\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.1, patience=1)\n",
    "    \n",
    "    max_features = params['max_features']\n",
    "    maxlen = params['max_len']\n",
    "    batch_size = params['batch_size']\n",
    "    embedding_dims = params['embedding_size']\n",
    "    filters = params['filters']\n",
    "    kernel_size = params['kernel_size']\n",
    "    hidden_dims = params['hidden_dims']\n",
    "    epochs = params['epochs']\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, embedding_dims, input_length=maxlen))\n",
    "    model.add(Dropout(0.2))\n",
    "    # we add a Convolution1D, which will learn filters\n",
    "    # word group filters of size filter_length:\n",
    "    model.add(Conv1D(filters, kernel_size, padding='valid', activation='relu', strides=1))\n",
    "    # we use max pooling:\n",
    "    model.add(Dense(hidden_dims))\n",
    "    # we add a Convolution1D, which will learn filters\n",
    "    # word group filters of size filter_length:\n",
    "    model.add(Conv1D(filters, kernel_size, padding='valid', activation='relu', strides=1))\n",
    "    # we use max pooling:\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    # We add a vanilla hidden layer:\n",
    "    model.add(Dense(hidden_dims))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Activation('relu'))\n",
    "    # We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    if fold_counter == 0:\n",
    "        model.summary()\n",
    "        \n",
    "    model.fit(X_tr, \n",
    "              y_tr, \n",
    "              batch_size=params['batch_size'],\n",
    "              epochs=params['epochs'],\n",
    "              validation_data=(X_te, y_te),\n",
    "              callbacks=[early_stopping],\n",
    "              verbose=0)\n",
    "\n",
    "    oof_train[te_index] = model.predict(X_te)[:, 0]\n",
    "    score = 100*metrics.roc_auc_score(y_valid, oof_valid_skf[fold_counter, :])\n",
    "    print('fold %d: [%.4f]' % (fold_counter+1, score))  \n",
    "\n",
    "    return oof_train, oof_valid_skf\n",
    "\n",
    "\n",
    "def model_LSTM(X_tr, X_te, y_tr, y_te, te_index, oof_train, oof_valid_skf, fold_counter, params):\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.1, patience=1)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(len(word2num), params['embedding_size']))\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss=params['loss_func'],\n",
    "                  optimizer=params['optimizer'],\n",
    "                  metrics=params['metrics'])\n",
    "    \n",
    "    if fold_counter == 0:\n",
    "        model.summary()\n",
    "        \n",
    "    model.fit(X_tr, \n",
    "              y_tr, \n",
    "              batch_size=params['batch_size'],\n",
    "              epochs=params['epochs'],\n",
    "              validation_data=(X_te, y_te),\n",
    "              callbacks=[early_stopping],\n",
    "              verbose=0)\n",
    "    \n",
    "    oof_train[te_index] = model.predict(X_te)[:, 0]\n",
    "    score = 100*metrics.roc_auc_score(y_valid, oof_valid_skf[fold_counter, :])\n",
    "    print('fold %d: [%.4f]' % (fold_counter+1, score))  \n",
    "\n",
    "    return oof_train, oof_valid_skf\n",
    "\n",
    "\n",
    "def model_CNNLSTM(X_tr, X_te, y_tr, y_te, te_index, oof_train, oof_valid_skf, fold_counter, params):\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.1, patience=1)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(len(word2num), params['embedding_size']))\n",
    "    model.add(Conv1D(filters=128, kernel_size=5, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Conv1D(filters=128, kernel_size=5, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dense(1, activation='sigmoid')) # sigmoid\n",
    "    \n",
    "    model.compile(loss=params['loss_func'],\n",
    "                  optimizer=params['optimizer'],\n",
    "                  metrics=params['metrics'])\n",
    "    \n",
    "    if fold_counter == 0:\n",
    "        model.summary()\n",
    "\n",
    "    model.fit(X_tr, \n",
    "              y_tr, \n",
    "              batch_size=params['batch_size'],\n",
    "              epochs=params['epochs'],\n",
    "              validation_data=(X_te, y_te),\n",
    "              callbacks=[early_stopping],\n",
    "              verbose=0)\n",
    "    \n",
    "    oof_train[te_index] = model.predict(X_te)[:, 0]\n",
    "    oof_valid_skf[fold_counter, :] = model.predict(X_valid)[:, 0]\n",
    "    score = 100*metrics.roc_auc_score(y_valid, oof_valid_skf[fold_counter, :])\n",
    "    print('fold %d: [%.4f]' % (fold_counter+1, score))\n",
    "\n",
    "    return oof_train, oof_valid_skf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_level_2 = np.zeros((df_train.shape[0], len(models) * len(features)))\n",
    "valid_level_2 = np.zeros((df_valid.shape[0], len(models) * len(features)))\n",
    "\n",
    "for feature_counter, feature in enumerate(features):\n",
    "    \n",
    "    params['max_len'] = feature['max_len']\n",
    "\n",
    "    # word to integer\n",
    "    X_train = np.array(word2int(df_train, n_u_words, feature['feature'], feature['max_len']))\n",
    "    X_valid = np.array(word2int(df_valid, n_u_words, feature['feature'], feature['max_len']))\n",
    "\n",
    "    ntrain = X_train.shape[0]\n",
    "    nvalid = X_valid.shape[0]\n",
    "\n",
    "    for model_counter, model in enumerate(models):\n",
    "        \n",
    "        idx = feature_counter*len(features) + model_counter\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"-\"*80)\n",
    "        print(\"Model {} of {}. {} architecture using feature '{}'\".format(idx+1, len(models)*len(features), model, feature['feature']))\n",
    "        print(\"-\"*80)\n",
    "\n",
    "        oof_train = np.zeros((ntrain,))\n",
    "        oof_valid = np.zeros((nvalid,))\n",
    "        oof_valid_skf = np.empty((NFOLDS, nvalid))\n",
    "\n",
    "        kf = StratifiedKFold(n_splits=NFOLDS, shuffle=False, random_state=0)\n",
    "\n",
    "        for fold_counter, (tr_index, te_index) in enumerate(kf.split(X_train, y_train)):\n",
    "\n",
    "            # Split data and target\n",
    "            X_tr = X_train[tr_index]\n",
    "            y_tr = y_train[tr_index]\n",
    "            X_te = X_train[te_index]\n",
    "            y_te = y_train[te_index]\n",
    "\n",
    "            if model == \"CNN\":\n",
    "                oof_train, oof_valid_skf = model_CNN(X_tr, X_te, y_tr, y_te, te_index, oof_train, oof_valid_skf, fold_counter, params)\n",
    "            elif model == \"LSTM\":\n",
    "                oof_train, oof_valid_skf = model_LSTM(X_tr, X_te, y_tr, y_te, te_index, oof_train, oof_valid_skf, fold_counter, params)\n",
    "            elif model == \"CNNLSTM\":\n",
    "                oof_train, oof_valid_skf = model_CNNLSTM(X_tr, X_te, y_tr, y_te, te_index, oof_train, oof_valid_skf, fold_counter, params)                \n",
    "\n",
    "        train_level_2[:, idx] = oof_train[:]\n",
    "        \n",
    "        print(\"\\nAveraging scores in out of fold valid dataset...\")\n",
    "        oof_valid[:] = oof_valid_skf.mean(axis=0)\n",
    "        valid_level_2[:, idx] = oof_valid[:]\n",
    "        score = 100*metrics.roc_auc_score(y_valid, oof_valid[:])\n",
    "        print('valid: [%.4f]' % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_level_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_train.reshape(-1, 1).shape 93.65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_valid.reshape(-1, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nAUC (total valid): {0:.2f}%\".format(100*metrics.roc_auc_score(y_valid, oof_valid[:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "    \n",
    "    for i, (train_index, test_index) in enumerate(kf):\n",
    "        x_tr = x_train[train_index]\n",
    "        y_tr = y_train[train_index]\n",
    "        x_te = x_train[test_index]\n",
    "\n",
    "        clf.train(x_tr, y_tr)\n",
    "\n",
    "        oof_train[test_index] = clf.predict(x_te)\n",
    "        oof_test_skf[i, :] = clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_evaluate_model(X_train, X_valid, y_train, y_valid, params):\n",
    "    \"\"\"Fit and evaluate Many to One RNN\n",
    "    \n",
    "    Args:\n",
    "        X_train    : Array with train features.\n",
    "        X_valid    : Array with validation features.\n",
    "        y_train    : Array with train flag.\n",
    "        y_valid    : Array with validation flag.\n",
    "        params     : Dictionary with parameter configuration.\n",
    "        \n",
    "    Returns:\n",
    "        model      : Model already trained.\n",
    "        pred_train : Array with train predictions.\n",
    "        pred_valid : Array with validation predictions.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\nCreating Sequential RNN: Many to One...\" )\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='loss', patience=2)\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Embedding(len(word2num), params['embedding_size'])) # , batch_size=batch_size\n",
    "    model.add(Conv1D(filters=128, kernel_size=5, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Conv1D(filters=128, kernel_size=5, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(LSTM(100))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='sigmoid')) # sigmoid\n",
    "    \n",
    "    model.compile(loss=params['loss_func'], optimizer=params['optimizer'], metrics=params['metrics'])\n",
    "    model.summary()\n",
    "\n",
    "    batch_size = params['batch_size']\n",
    "    print(\"\\nFitting the model ...\" )\n",
    "    model.fit(X_train, y_train, batch_size=batch_size, epochs=params['epochs'], callbacks=[early_stopping])\n",
    "    \n",
    "    print(\"\\nPredicting probs on train ...\" )\n",
    "    pred_train = model.predict(X_train)\n",
    "    print(\"\\nAUC: {0:.2f}%\".format(100*metrics.roc_auc_score(y_train, pred_train)), \"| GINI: {0:.2f}%\".format(ip(y_train, pred_train)))\n",
    "\n",
    "    print(\"\\nEvaluating in valid ...\" )\n",
    "    print(model.evaluate(X_valid, y_valid, batch_size=batch_size))\n",
    "    \n",
    "    print(\"\\nPredicting probs on valid ...\" )\n",
    "    pred_valid = model.predict(X_valid)\n",
    "    print(\"\\nAUC: {0:.2f}%\".format(100*metrics.roc_auc_score(y_valid, pred_valid)), \"| GINI: {0:.2f}%\".format(ip(y_valid, pred_valid)))\n",
    "\n",
    "    return model, pred_train, pred_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_threshold = 500\n",
    "\n",
    "params = {\n",
    "    'loss_func': 'binary_crossentropy', # binary_crossentropy\n",
    "    'optimizer': 'rmsprop', # adam, rmsprop\n",
    "    'metrics': ['accuracy'],\n",
    "    'embedding_size': 100,\n",
    "    'batch_size': 128,\n",
    "    'epochs': 3\n",
    "}\n",
    "\n",
    "# word to integers\n",
    "print(\"\\nTrain\")\n",
    "X_train = np.array(word2int(df_train, n_u_words, 'text', word_threshold))\n",
    "print(\"\\nValid\")\n",
    "X_valid = np.array(word2int(df_valid, n_u_words, 'text', word_threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_text, pred_train, pred_valid = fit_evaluate_model(X_train, X_valid, y_train, y_valid, params)\n",
    "\n",
    "print(\"\\nTest results ...\" )\n",
    "pred_test = predict_test(model_text, df_test, 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['pred_text'] = pred_train\n",
    "df_valid['pred_text'] = pred_valid\n",
    "df_test['pred_text'] = pred_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_threshold = 15\n",
    "\n",
    "params = {\n",
    "    'loss_func': 'binary_crossentropy', # binary_crossentropy\n",
    "    'optimizer': 'rmsprop', # adam, rmsprop\n",
    "    'metrics': ['accuracy'],\n",
    "    'embedding_size': 100,\n",
    "    'batch_size': 128,\n",
    "    'epochs': 3\n",
    "}\n",
    "\n",
    "# word to integer\n",
    "X_train = np.array(word2int(df_train, n_u_words, 'title', word_threshold))\n",
    "X_valid = np.array(word2int(df_valid, n_u_words, 'title', word_threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_title, pred_train, pred_valid = fit_evaluate_model(X_train, X_valid, y_train, y_valid, params)\n",
    "\n",
    "print(\"\\nTest results ...\" )\n",
    "pred_test = predict_test(model_title, df_test, 'title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['pred_title'] = pred_train\n",
    "df_valid['pred_title'] = pred_valid\n",
    "df_test['pred_title'] = pred_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_threshold = 250\n",
    "\n",
    "params = {\n",
    "    'loss_func': 'binary_crossentropy', # binary_crossentropy\n",
    "    'optimizer': 'rmsprop', # adam, rmsprop\n",
    "    'metrics': ['accuracy'],\n",
    "    'embedding_size': 100,\n",
    "    'batch_size': 128,\n",
    "    'epochs': 3\n",
    "}\n",
    "\n",
    "# word to integer\n",
    "X_train = np.array(word2int(df_train, n_u_words, 'summary', word_threshold))\n",
    "X_valid = np.array(word2int(df_valid, n_u_words, 'summary', word_threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_summary, pred_train, pred_valid = fit_evaluate_model(X_train, X_valid, y_train, y_valid, params)\n",
    "\n",
    "print(\"\\nTest results ...\" )\n",
    "pred_test = predict_test(model_summary, df_test, 'summary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['pred_summary'] = pred_train\n",
    "df_valid['pred_summary'] = pred_valid\n",
    "df_test['pred_summary'] = pred_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_evaluate_xgboost(alg, \n",
    "             dtrain, \n",
    "             dtest, \n",
    "             predictors, \n",
    "             verbose=0, \n",
    "             useTrainCV=True, \n",
    "             cv_folds=5, \n",
    "             early_stopping_rounds=50, \n",
    "             flag='flag'):\n",
    "    \n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(dtrain[predictors].values, label=dtrain[flag].values.flatten())\n",
    "        cvresult = xgb.cv(\n",
    "            xgb_param, \n",
    "            xgtrain, \n",
    "            num_boost_round=alg.get_params()['n_estimators'], \n",
    "            nfold=cv_folds,\n",
    "            metrics='auc', \n",
    "            early_stopping_rounds=early_stopping_rounds, \n",
    "            verbose_eval=verbose)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "        print(alg.get_params())\n",
    "    \n",
    "    # Fit the algorithm on the data\n",
    "    alg.fit(dtrain[predictors], dtrain[flag].values.flatten(),eval_metric='auc')\n",
    "        \n",
    "    # Predict training set:\n",
    "    dtrain_predictions = alg.predict(dtrain[predictors])\n",
    "    dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n",
    "        \n",
    "    # Print model report:\n",
    "    print( \"\\nModel Report (Train)\")\n",
    "    print( \"Accuracy : %.4g\" % metrics.accuracy_score(dtrain[flag].values, dtrain_predictions))\n",
    "    print( \"AUC Score: %f\" % metrics.roc_auc_score(dtrain[flag].values, dtrain_predprob))\n",
    "    \n",
    "    # Predict validation set:\n",
    "    dtest_predprob = alg.predict_proba(dtest[predictors])[:,1]\n",
    "\n",
    "    # Print model report:\n",
    "    print( \"\\nModel Report (Test)\")\n",
    "    print( \"AUC Score: %f\" % metrics.roc_auc_score(dtest[flag].values, dtest_predprob))\n",
    "    \n",
    "    return alg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = ['pred_text', 'pred_title', 'pred_summary']\n",
    "\n",
    "# xgb sparse matrix\n",
    "xgtrain = xgb.DMatrix(X_train, label= y_train)\n",
    "xgvalid = xgb.DMatrix(X_valid, label= y_valid)\n",
    "\n",
    "model_xgb = XGBClassifier(\n",
    " booster = 'gbtree',\n",
    " learning_rate =0.01,\n",
    " n_estimators=1000,\n",
    " max_depth=5,\n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " objective= 'binary:logistic',\n",
    " nthread=4,\n",
    " reg_alpha=1,\n",
    " scale_pos_weight=1,\n",
    " seed=27)\n",
    "\n",
    "model_xgb = fit_evaluate_xgboost(model_xgb, df_train, df_valid, predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, model_name):\n",
    "    print(\"\\nSaving\", model_name,\"...\")\n",
    "    model.save(\"./models/%s.h5\" % model_name)\n",
    "    \n",
    "save_model(model_text, \"model_text\")\n",
    "save_model(model_title, \"model_title\")\n",
    "save_model(model_summary, \"model_summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model to file\n",
    "pickle.dump(model_xgb, open(\"./models/model_xgb.pickle.dat\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model_summary.save('./models/model_summary.h5')\n",
    "model = load_model('./models/model_summary.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.roc_auc_score(y_valid, model.predict_proba(X_valid)[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_threshold = 500\n",
    "X_train = np.array(word2int(df_train, n_u_words, 'text', word_threshold))\n",
    "X_valid = np.array(word2int(df_valid, n_u_words, 'text', word_threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_instance = X_valid[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.transpose(X_train).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_txt = pd.DataFrame(X_train)\n",
    "df_valid_txt = pd.DataFrame(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime\n",
    "import lime.lime_tabular\n",
    "\n",
    "# create the lime explainer\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(df_train_txt.as_matrix(), feature_names=df_train_txt.columns) # X_train.values, , class_names=(0,1)\n",
    "\n",
    "predict_fn = lambda x: model_text.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = explainer.explain_instance(np.reshape(df_valid_txt.loc[1], 500), predict_fn, num_features=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lime\n",
    "# ------------------------------------------------------------------------------\n",
    "print('\\nUsing Lime to explain instances...')\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "import re\n",
    "\n",
    "# create the lime explainer\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(df_train[features].as_matrix(), feature_names=features) # X_train.values, , class_names=(0,1)\n",
    "\n",
    "def lime_explain_instance(id):\n",
    "\n",
    "    test_instance_tot = test.loc[test[col_id]==id].head(1)\n",
    "    test_instance = test_instance_tot[features]\n",
    "    test_instance = test_instance.clip(-10000000.0, 10000000.0) # convert int to float instead?\n",
    "    test_instance = test_instance.values[0]\n",
    "\n",
    "    # prediction function: for classifiers, this should be function that takes a numpy array and outputs probability predictions\n",
    "    predict_fn_xgb = lambda x: clf.predict_proba(x).astype(float)\n",
    "\n",
    "    exp = explainer.explain_instance(test_instance, predict_fn_xgb, num_features=200) # test_instance.values\n",
    "    print('Document id     : %d' % (id))\n",
    "    print('Probability (=1):', clf.predict_proba([test_instance])[0,1])\n",
    "    print('True class      : %s' % test_instance_tot[col_target].values[0])\n",
    "\n",
    "    ll = []\n",
    "    for i in range(1, len(exp.as_list()), 1):\n",
    "        id_var = exp.as_map()[1][i][0]\n",
    "        var = features[id_var]\n",
    "        value = test_instance[id_var]\n",
    "        crit = exp.as_list()[i][0]\n",
    "        w = exp.as_list()[i][1]\n",
    "        dd = {\n",
    "            \"variable\": var,\n",
    "            \"value\": value,\n",
    "            \"explanation\": w,\n",
    "            \"criteria\": crit\n",
    "        }\n",
    "        ll.append(dd)\n",
    "\n",
    "    explainer_df = pd.DataFrame(ll)\n",
    "    explainer_df = explainer_df.sort_values('explanation', ascending=False)\n",
    "    explainer_df.head(10)\n",
    "    explainer_df.tail(10)\n",
    "\n",
    "    pyplot.bar(range(len(explainer_df)), explainer_df['explanation'].values)\n",
    "    ind = np.arange(len(explainer_df['variable'].values))    # the x locations for the groups\n",
    "    pyplot.xticks(ind, explainer_df['variable'].values, rotation='vertical')\n",
    "    # pyplot.savefig('3_gbm_raw_feature_importance.png', bbox_inches='tight')\n",
    "    pyplot.show()\n",
    "\n",
    "    return explainer_df\n",
    "\n",
    "# check top 15 of largest estimated probabilities\n",
    "test[['id', 'TARGET', 'predprob']].sort_values('predprob', ascending=False).head(15)\n",
    "\n",
    "\"\"\"\n",
    "2016030520890380\n",
    "2014120519399710\n",
    "2015120012335320\n",
    "2015060519288510\n",
    "2015090014583910\n",
    "2014120013445730\n",
    "\"\"\"\n",
    "explainer_df = lime_explain_instance(2016030520890380)\n",
    "\n",
    "explainer_df.head(10)\n",
    "explainer_df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission = df_test[['id', 'pred']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_time = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print submission_time\n",
    "df_submission.to_csv('../submissions/submission_%s.csv' % submission_time, sep=\",\", na_rep=\"\", mode=\"w\", index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
