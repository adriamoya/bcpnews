{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/moyandreu/coding/bcpnews/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: iso-8859-15 -*-\n",
    "\n",
    "# RRN to classify text\n",
    "# Author: adriamoya\n",
    "\n",
    "#%matplotlib inline\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "import pickle\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import random as rn\n",
    "import tensorflow as tf\n",
    "\n",
    "# The below is necessary for starting Numpy generated random numbers\n",
    "# in a well-defined initial state.\n",
    "np.random.seed(1337)\n",
    "\n",
    "# The below is necessary for starting core Python generated random numbers\n",
    "# in a well-defined state.\n",
    "rn.seed(12345)\n",
    "\n",
    "# Force TensorFlow to use single thread.\n",
    "# Multiple threads are a potential source of\n",
    "# non-reproducible results.\n",
    "# For further details, see: https://stackoverflow.com/questions/42022950/which-seeds-have-to-be-set-where-to-realize-100-reproducibility-of-training-res\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "\n",
    "from keras import backend as K\n",
    "# The below tf.set_random_seed() will make random number generation\n",
    "# in the TensorFlow backend have a well-defined initial state.\n",
    "# For further details, see: https://www.tensorflow.org/api_docs/python/tf/set_random_seed\n",
    "\n",
    "tf.set_random_seed(1234)\n",
    "\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)\n",
    "\n",
    "# Rest of code follows ...\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, BatchNormalization, LSTM, Embedding, Reshape, Conv1D, MaxPooling1D\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import warnings\n",
    "pd.options.mode.chained_assignment = None\n",
    "warnings.filterwarnings(module='sklearn*', action='ignore', category=DeprecationWarning)\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables\n",
    "flag = 'flag'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df = pd.read_csv(\"../../1_construction/3_newspaper_scraper/analyses/cleaned_datasets/train.csv\")\n",
    "df_test  = pd.read_csv(\"../../1_construction/3_newspaper_scraper/analyses/cleaned_datasets/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ip calculation\n",
    "def ip(y_target, y_pred):\n",
    "    return 100*(2*(metrics.roc_auc_score(y_target, y_pred))-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df, column=\"text\"):\n",
    "    \"\"\"Preprocessing (lower case, remove urls, punctuations).\n",
    "    \n",
    "    Args:\n",
    "        df     : Dataset with articles information (pandas.DataFrame).\n",
    "        column : Name of the column that contains the text of the article. Default is `text`.\n",
    "        \n",
    "    Returns:\n",
    "        df     : Dataset with articles information (pandas.DataFrame).\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\nPreprocessing %s ...\" % (column))\n",
    "\n",
    "    # preprocessing steps: lower case, remove urls, punctuations ...\n",
    "    df[column] = df[column].str.lower()\n",
    "    df[column] = df[column].str.replace(r'http[\\w:/\\.]+','') # remove urls\n",
    "    df[column] = df[column].str.replace(r'[^\\.(a-zA-ZÀ-ÿ0-9)\\s]','') #remove everything but characters and punctuation ( [^\\.\\w\\s] )\n",
    "    df[column] = df[column].str.replace(r'(?<=\\d)(\\.)(?=\\d)','') #remove dots in thousands (careful with decimals!)\n",
    "    df[column] = df[column].str.replace(r'\\.\\.+','.') #replace multple periods with a single one\n",
    "    df[column] = df[column].str.replace(r'\\.',' .') #replace multple periods with a single one\n",
    "    df[column] = df[column].str.replace(r'\\(',' ') # replace brackets with white spaces\n",
    "    df[column] = df[column].str.replace(r'\\)',' ') # replace brackets with white spaces\n",
    "    df[column] = df[column].str.replace(r'\\s\\s+',' ') #replace multple white space with a single one\n",
    "    df[column] = df[column].str.strip()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dictionary(df, min_count_word=5):\n",
    "    \"\"\"Build dictionary and relationships between words and integers.\n",
    "    \n",
    "    Args:\n",
    "        df             : Dataset with articles information (pandas.DataFrame).\n",
    "        min_count_word : Only consider words that have been used more than n times. Default is 5.\n",
    "        \n",
    "    Returns:\n",
    "        word2num       : Dictionary (words to numbers).\n",
    "        num2word       : Dictionary (numbers to words).\n",
    "        n_u_words      : Length of the dictionary (number of unique words).\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\nBuilding dictionary ...\" )\n",
    "\n",
    "    # get all unique words (only consider words that have been used more than 5 times)\n",
    "    all_text = ' '.join(df.text.values)\n",
    "    words = all_text.split()\n",
    "    u_words = Counter(words).most_common()\n",
    "    u_words = [word[0] for word in u_words if word[1]>min_count_word] # we will only consider words that have been used more than 5 times\n",
    "\n",
    "    print('The number of unique words is:', \"{:,}\".format(len(u_words)))\n",
    "\n",
    "    # create the dictionary\n",
    "    word2num = dict(zip(u_words,range(len(u_words))))\n",
    "    word2num['<Other>'] = len(u_words)\n",
    "    num2word = dict(zip(word2num.values(), word2num.keys()))\n",
    "\n",
    "    num2word[len(word2num)] = '<PAD>'\n",
    "    word2num['<PAD>'] = len(word2num)\n",
    "    \n",
    "    n_u_words = len(u_words)\n",
    "\n",
    "    return word2num, num2word, n_u_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2int(df, n_u_words, column='text', word_threshold=500):\n",
    "    \"\"\"Convert words to integers and prepad sentences\n",
    "    \n",
    "    Args:\n",
    "        df             : Dataset with articles information (pandas.DataFrame)\n",
    "        n_u_words      : Length of the dictionary (number of unique words).\n",
    "        column         : Name of the column that contains the text of the article. Default is `text`.\n",
    "        word_threshold : Number of words to consider for each text (padding). Default is 500.\n",
    "        \n",
    "    Returns:\n",
    "        int_text       : Array with texts translated to integers.\n",
    "        \"\"\"\n",
    "\n",
    "    print(\"\\nConverting words to integers and prepadding ...\" )\n",
    "\n",
    "    int_text = [[word2num[word] if word in word2num else n_u_words for word in Text.split()] for Text in df[column].values] # Text.split() python2\n",
    "\n",
    "    print('The number of texts greater than %s in length is: ' % str(word_threshold), \"{:,}\".format(np.sum(np.array([len(t)>word_threshold for t in int_text]))))\n",
    "    print('The number of texts less than 50 in length is: ', \"{:,}\".format(np.sum(np.array([len(t)<50 for t in int_text]))))\n",
    "\n",
    "    for i, t in enumerate(int_text):\n",
    "        if len(t)<word_threshold:\n",
    "            int_text[i] = [word2num['<PAD>']]*(word_threshold-len(t)) + t\n",
    "        elif len(t)>word_threshold:\n",
    "            int_text[i] = t[:word_threshold]\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    return int_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test(model, X_test, column):\n",
    "    \"\"\"Make predictions in test dataset.\n",
    "    \n",
    "    Args:\n",
    "        model     : Model trained.\n",
    "        X_test    : Array with test features.\n",
    "        column    : Name of the column that contains desired feature.\n",
    "        \n",
    "    Returns:\n",
    "        pred_test : Array with test predictions.\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    # words to numbers\n",
    "    int_text = word2int(X_test, n_u_words, column, word_threshold)\n",
    "\n",
    "    X = np.array(int_text)\n",
    "\n",
    "    pred = model.predict(X)\n",
    "\n",
    "    l_pred = []\n",
    "    for item in pred:\n",
    "        l_pred.append(item[0])\n",
    "        \n",
    "    return l_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessing text ...\n",
      "\n",
      "Preprocessing text ...\n",
      "\n",
      "Preprocessing title ...\n",
      "\n",
      "Preprocessing title ...\n",
      "\n",
      "Preprocessing summary ...\n",
      "\n",
      "Preprocessing summary ...\n"
     ]
    }
   ],
   "source": [
    "# preprocessing steps: lower case, remove urls, punctuations ...\n",
    "\n",
    "# text\n",
    "df = preprocessing(df, 'text')\n",
    "df_test = preprocessing(df_test, 'text')\n",
    "\n",
    "# title\n",
    "df = preprocessing(df, 'title')\n",
    "df_test = preprocessing(df_test, 'title')\n",
    "\n",
    "# summary\n",
    "df = preprocessing(df, 'summary')\n",
    "df_test = preprocessing(df_test, 'summary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building dictionary ...\n",
      "The number of unique words is: 41,112\n"
     ]
    }
   ],
   "source": [
    "# build dictionary\n",
    "min_count_word = 4\n",
    "word2num, num2word, n_u_words = build_dictionary(df, min_count_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train / Validation split ...\n",
      "X_train: (11709, 6)\n",
      "X_valid: (2928, 6)\n",
      "y_train: (11709,)\n",
      "y_valid: (2928,)\n"
     ]
    }
   ],
   "source": [
    "# train / validation split\n",
    "print(\"\\nTrain / Validation split ...\")\n",
    "\n",
    "X, y = df[df.columns[~df.columns.str.contains(flag)]].values, df[flag].values\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, stratify=y, shuffle=True, random_state=42)\n",
    "\n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"X_valid:\", X_valid.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"y_valid:\", y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = df.columns[~df.columns.str.contains(flag)].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(X_train, columns=columns); df_train[flag] = y_train\n",
    "df_valid = pd.DataFrame(X_valid, columns=columns); df_valid[flag] = y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (11709, 7)\n",
      "Valid: (2928, 7)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train:\", df_train.shape)\n",
    "print(\"Valid:\", df_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_CNNLSTM(X_train, y_train, X_valid, y_valid, params):\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='loss', patience=2)\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Embedding(len(word2num), params['embedding_size'])) # , batch_size=batch_size\n",
    "    model.add(Conv1D(filters=128, kernel_size=5, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Conv1D(filters=128, kernel_size=5, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dense(1, activation='sigmoid')) # sigmoid\n",
    "    \n",
    "    model.compile(loss=params['loss_func'], optimizer=params['optimizer'], metrics=params['metrics'])\n",
    "    model.summary()\n",
    "\n",
    "    batch_size = params['batch_size']\n",
    "    model.fit(X_train, y_train, batch_size=batch_size, epochs=params['epochs'], callbacks=[early_stopping])\n",
    "    pred_train = model.predict(X_train)\n",
    "    pred_valid = model.predict(X_valid)\n",
    "    \n",
    "    print(\"\\nAUC (train): {0:.2f}%\".format(100*metrics.roc_auc_score(y_train, pred_train)))\n",
    "    print(\"\\nAUC (valid): {0:.2f}%\".format(100*metrics.roc_auc_score(y_valid, pred_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Converting words to integers and prepadding ...\n",
      "The number of texts greater than 15 in length is:  1,957\n",
      "The number of texts less than 50 in length is:  11,709\n",
      "\n",
      "Converting words to integers and prepadding ...\n",
      "The number of texts greater than 15 in length is:  466\n",
      "The number of texts less than 50 in length is:  2,928\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Fold 0\n",
      "--------------------------------------------------------------------------------\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 100)         4111400   \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, None, 128)         64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, None, 128)         82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               91600     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 4,349,277\n",
      "Trainable params: 4,349,277\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "9366/9366 [==============================] - 8s 872us/step - loss: 0.4300 - acc: 0.8073\n",
      "Epoch 2/3\n",
      "9366/9366 [==============================] - 7s 789us/step - loss: 0.2396 - acc: 0.9050\n",
      "Epoch 3/3\n",
      "9366/9366 [==============================] - 7s 796us/step - loss: 0.1798 - acc: 0.9318\n",
      "\n",
      "AUC (train): 98.76%\n",
      "\n",
      "AUC (valid): 93.22%\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Fold 1\n",
      "--------------------------------------------------------------------------------\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, None, 100)         4111400   \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, None, 128)         64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, None, 128)         82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               91600     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 4,349,277\n",
      "Trainable params: 4,349,277\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "9366/9366 [==============================] - 9s 914us/step - loss: 0.4212 - acc: 0.8153\n",
      "Epoch 2/3\n",
      "9366/9366 [==============================] - 7s 788us/step - loss: 0.2314 - acc: 0.9083\n",
      "Epoch 3/3\n",
      "9366/9366 [==============================] - 8s 812us/step - loss: 0.1708 - acc: 0.9349\n",
      "\n",
      "AUC (train): 98.67%\n",
      "\n",
      "AUC (valid): 92.88%\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Fold 2\n",
      "--------------------------------------------------------------------------------\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, None, 100)         4111400   \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, None, 128)         64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, None, 128)         82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 100)               91600     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 4,349,277\n",
      "Trainable params: 4,349,277\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "9368/9368 [==============================] - 9s 967us/step - loss: 0.4329 - acc: 0.8043\n",
      "Epoch 2/3\n",
      "9368/9368 [==============================] - 7s 790us/step - loss: 0.2320 - acc: 0.9071\n",
      "Epoch 3/3\n",
      "9368/9368 [==============================] - 8s 875us/step - loss: 0.1763 - acc: 0.9350\n",
      "\n",
      "AUC (train): 98.72%\n",
      "\n",
      "AUC (valid): 91.81%\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Fold 3\n",
      "--------------------------------------------------------------------------------\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, None, 100)         4111400   \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, None, 128)         64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, None, 128)         82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 100)               91600     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 4,349,277\n",
      "Trainable params: 4,349,277\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "9368/9368 [==============================] - 9s 959us/step - loss: 0.4159 - acc: 0.8187\n",
      "Epoch 2/3\n",
      "9368/9368 [==============================] - 8s 843us/step - loss: 0.2256 - acc: 0.9133\n",
      "Epoch 3/3\n",
      "9368/9368 [==============================] - 8s 848us/step - loss: 0.1700 - acc: 0.9340\n",
      "\n",
      "AUC (train): 98.79%\n",
      "\n",
      "AUC (valid): 92.05%\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Fold 4\n",
      "--------------------------------------------------------------------------------\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, None, 100)         4111400   \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, None, 128)         64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, None, 128)         82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 100)               91600     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 4,349,277\n",
      "Trainable params: 4,349,277\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "9368/9368 [==============================] - 10s 1ms/step - loss: 0.4291 - acc: 0.8094\n",
      "Epoch 2/3\n",
      "9368/9368 [==============================] - 7s 792us/step - loss: 0.2341 - acc: 0.9098\n",
      "Epoch 3/3\n",
      "9368/9368 [==============================] - 7s 789us/step - loss: 0.1749 - acc: 0.9327\n",
      "\n",
      "AUC (train): 98.82%\n",
      "\n",
      "AUC (valid): 91.74%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "models = ['CNN', 'LSTM', 'BiLSTM', 'CNNLSTM']\n",
    "\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=False, random_state=0)\n",
    "\n",
    "word_threshold = 15\n",
    "\n",
    "params = {\n",
    "    'loss_func': 'binary_crossentropy', # binary_crossentropy\n",
    "    'optimizer': 'rmsprop', # adam, rmsprop\n",
    "    'metrics': ['accuracy'],\n",
    "    'embedding_size': 100,\n",
    "    'batch_size': 128,\n",
    "    'epochs': 3\n",
    "}\n",
    "\n",
    "X_train = np.array(word2int(df_train, n_u_words, 'title', word_threshold))\n",
    "X_valid = np.array(word2int(df_valid, n_u_words, 'title', word_threshold))\n",
    "\n",
    "for fold_counter, (tr_index, te_index) in enumerate(kf.split(X_train, y_train)):\n",
    "    print(\"\")\n",
    "    print(\"-\"*80)\n",
    "    print(\"Fold %d\" % fold_counter)\n",
    "    print(\"-\"*80)\n",
    "    X_tr = X_train[tr_index]; y_tr = y_train[tr_index]\n",
    "    X_te = X_train[te_index]; y_te = y_train[te_index]\n",
    "    \n",
    "    model_CNNLSTM(X_tr, y_tr, X_te, y_te, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_evaluate_model(X_train, X_valid, y_train, y_valid, params):\n",
    "    \"\"\"Fit and evaluate Many to One RNN\n",
    "    \n",
    "    Args:\n",
    "        X_train    : Array with train features.\n",
    "        X_valid    : Array with validation features.\n",
    "        y_train    : Array with train flag.\n",
    "        y_valid    : Array with validation flag.\n",
    "        params     : Dictionary with parameter configuration.\n",
    "        \n",
    "    Returns:\n",
    "        model      : Model already trained.\n",
    "        pred_train : Array with train predictions.\n",
    "        pred_valid : Array with validation predictions.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\nCreating Sequential RNN: Many to One...\" )\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='loss', patience=2)\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Embedding(len(word2num), params['embedding_size'])) # , batch_size=batch_size\n",
    "    model.add(Conv1D(filters=128, kernel_size=5, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Conv1D(filters=128, kernel_size=5, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(LSTM(100))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='sigmoid')) # sigmoid\n",
    "    \n",
    "    model.compile(loss=params['loss_func'], optimizer=params['optimizer'], metrics=params['metrics'])\n",
    "    model.summary()\n",
    "\n",
    "    batch_size = params['batch_size']\n",
    "    print(\"\\nFitting the model ...\" )\n",
    "    model.fit(X_train, y_train, batch_size=batch_size, epochs=params['epochs'], callbacks=[early_stopping])\n",
    "    \n",
    "    print(\"\\nPredicting probs on train ...\" )\n",
    "    pred_train = model.predict(X_train)\n",
    "    print(\"\\nAUC: {0:.2f}%\".format(100*metrics.roc_auc_score(y_train, pred_train)), \"| GINI: {0:.2f}%\".format(ip(y_train, pred_train)))\n",
    "\n",
    "    print(\"\\nEvaluating in valid ...\" )\n",
    "    print(model.evaluate(X_valid, y_valid, batch_size=batch_size))\n",
    "    \n",
    "    print(\"\\nPredicting probs on valid ...\" )\n",
    "    pred_valid = model.predict(X_valid)\n",
    "    print(\"\\nAUC: {0:.2f}%\".format(100*metrics.roc_auc_score(y_valid, pred_valid)), \"| GINI: {0:.2f}%\".format(ip(y_valid, pred_valid)))\n",
    "\n",
    "    return model, pred_train, pred_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train\n",
      "\n",
      "Converting words to integers and prepadding ...\n",
      "The number of texts greater than 500 in length is:  5,372\n",
      "The number of texts less than 50 in length is:  130\n",
      "\n",
      "Valid\n",
      "\n",
      "Converting words to integers and prepadding ...\n",
      "The number of texts greater than 500 in length is:  1,336\n",
      "The number of texts less than 50 in length is:  33\n"
     ]
    }
   ],
   "source": [
    "word_threshold = 500\n",
    "\n",
    "params = {\n",
    "    'loss_func': 'binary_crossentropy', # binary_crossentropy\n",
    "    'optimizer': 'rmsprop', # adam, rmsprop\n",
    "    'metrics': ['accuracy'],\n",
    "    'embedding_size': 100,\n",
    "    'batch_size': 128,\n",
    "    'epochs': 3\n",
    "}\n",
    "\n",
    "# word to integers\n",
    "print(\"\\nTrain\")\n",
    "X_train = np.array(word2int(df_train, n_u_words, 'text', word_threshold))\n",
    "print(\"\\nValid\")\n",
    "X_valid = np.array(word2int(df_valid, n_u_words, 'text', word_threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating Sequential RNN: Many to One...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 100)         4111400   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, None, 128)         64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, None, 128)         82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               91600     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 4,349,277\n",
      "Trainable params: 4,349,277\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Fitting the model ...\n",
      "Epoch 1/3\n",
      "11709/11709 [==============================] - 152s 13ms/step - loss: 0.4283 - acc: 0.8136\n",
      "Epoch 2/3\n",
      "11709/11709 [==============================] - 147s 13ms/step - loss: 0.2504 - acc: 0.9037\n",
      "Epoch 3/3\n",
      "11709/11709 [==============================] - 145s 12ms/step - loss: 0.1939 - acc: 0.9247\n",
      "\n",
      "Predicting probs on train ...\n",
      "\n",
      "AUC: 98.71% | GINI: 97.42%\n",
      "\n",
      "Evaluating in valid ...\n",
      "2928/2928 [==============================] - 11s 4ms/step\n",
      "[0.3779788886914488, 0.8541666660152498]\n",
      "\n",
      "Predicting probs on valid ...\n",
      "\n",
      "AUC: 92.06% | GINI: 84.12%\n",
      "\n",
      "Test results ...\n",
      "\n",
      "Converting words to integers and prepadding ...\n",
      "The number of texts greater than 500 in length is:  2,654\n",
      "The number of texts less than 50 in length is:  45\n"
     ]
    }
   ],
   "source": [
    "model_text, pred_train, pred_valid = fit_evaluate_model(X_train, X_valid, y_train, y_valid, params)\n",
    "\n",
    "print(\"\\nTest results ...\" )\n",
    "pred_test = predict_test(model_text, df_test, 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['pred_text'] = pred_train\n",
    "df_valid['pred_text'] = pred_valid\n",
    "df_test['pred_text'] = pred_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Converting words to integers and prepadding ...\n",
      "The number of texts greater than 15 in length is:  1,957\n",
      "The number of texts less than 50 in length is:  11,709\n",
      "\n",
      "Converting words to integers and prepadding ...\n",
      "The number of texts greater than 15 in length is:  466\n",
      "The number of texts less than 50 in length is:  2,928\n"
     ]
    }
   ],
   "source": [
    "word_threshold = 15\n",
    "\n",
    "params = {\n",
    "    'loss_func': 'binary_crossentropy', # binary_crossentropy\n",
    "    'optimizer': 'rmsprop', # adam, rmsprop\n",
    "    'metrics': ['accuracy'],\n",
    "    'embedding_size': 100,\n",
    "    'batch_size': 128,\n",
    "    'epochs': 3\n",
    "}\n",
    "\n",
    "# word to integer\n",
    "X_train = np.array(word2int(df_train, n_u_words, 'title', word_threshold))\n",
    "X_valid = np.array(word2int(df_valid, n_u_words, 'title', word_threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating Sequential RNN: Many to One...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 100)         4111400   \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, None, 128)         64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, None, 128)         82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               91600     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 4,349,277\n",
      "Trainable params: 4,349,277\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Fitting the model ...\n",
      "Epoch 1/3\n",
      "11709/11709 [==============================] - 11s 897us/step - loss: 0.4021 - acc: 0.8234\n",
      "Epoch 2/3\n",
      "11709/11709 [==============================] - 10s 817us/step - loss: 0.2284 - acc: 0.9113\n",
      "Epoch 3/3\n",
      "11709/11709 [==============================] - 10s 827us/step - loss: 0.1778 - acc: 0.9325\n",
      "\n",
      "Predicting probs on train ...\n",
      "\n",
      "AUC: 98.60% | GINI: 97.21%\n",
      "\n",
      "Evaluating in valid ...\n",
      "2928/2928 [==============================] - 0s 153us/step\n",
      "[0.2890244030235895, 0.8855874313682807]\n",
      "\n",
      "Predicting probs on valid ...\n",
      "\n",
      "AUC: 93.78% | GINI: 87.56%\n",
      "\n",
      "Test results ...\n",
      "\n",
      "Converting words to integers and prepadding ...\n",
      "The number of texts greater than 15 in length is:  1,373\n",
      "The number of texts less than 50 in length is:  5,225\n"
     ]
    }
   ],
   "source": [
    "model_title, pred_train, pred_valid = fit_evaluate_model(X_train, X_valid, y_train, y_valid, params)\n",
    "\n",
    "print(\"\\nTest results ...\" )\n",
    "pred_test = predict_test(model_title, df_test, 'title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['pred_title'] = pred_train\n",
    "df_valid['pred_title'] = pred_valid\n",
    "df_test['pred_title'] = pred_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Converting words to integers and prepadding ...\n",
      "The number of texts greater than 250 in length is:  39\n",
      "The number of texts less than 50 in length is:  160\n",
      "\n",
      "Converting words to integers and prepadding ...\n",
      "The number of texts greater than 250 in length is:  8\n",
      "The number of texts less than 50 in length is:  35\n"
     ]
    }
   ],
   "source": [
    "word_threshold = 250\n",
    "\n",
    "params = {\n",
    "    'loss_func': 'binary_crossentropy', # binary_crossentropy\n",
    "    'optimizer': 'rmsprop', # adam, rmsprop\n",
    "    'metrics': ['accuracy'],\n",
    "    'embedding_size': 100,\n",
    "    'batch_size': 128,\n",
    "    'epochs': 3\n",
    "}\n",
    "\n",
    "# word to integer\n",
    "X_train = np.array(word2int(df_train, n_u_words, 'summary', word_threshold))\n",
    "X_valid = np.array(word2int(df_valid, n_u_words, 'summary', word_threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating Sequential RNN: Many to One...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, None, 100)         4111400   \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, None, 128)         64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, None, 128)         82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               91600     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 4,349,277\n",
      "Trainable params: 4,349,277\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Fitting the model ...\n",
      "Epoch 1/3\n",
      "11709/11709 [==============================] - 77s 7ms/step - loss: 0.5079 - acc: 0.7677\n",
      "Epoch 2/3\n",
      "11709/11709 [==============================] - 77s 7ms/step - loss: 0.2985 - acc: 0.8768\n",
      "Epoch 3/3\n",
      "11709/11709 [==============================] - 76s 6ms/step - loss: 0.2347 - acc: 0.9078\n",
      "\n",
      "Predicting probs on train ...\n",
      "\n",
      "AUC: 97.85% | GINI: 95.70%\n",
      "\n",
      "Evaluating in valid ...\n",
      "2928/2928 [==============================] - 6s 2ms/step\n",
      "[0.3867515235976443, 0.854508196395603]\n",
      "\n",
      "Predicting probs on valid ...\n",
      "\n",
      "AUC: 89.83% | GINI: 79.65%\n",
      "\n",
      "Test results ...\n",
      "\n",
      "Converting words to integers and prepadding ...\n",
      "The number of texts greater than 250 in length is:  28\n",
      "The number of texts less than 50 in length is:  56\n"
     ]
    }
   ],
   "source": [
    "model_summary, pred_train, pred_valid = fit_evaluate_model(X_train, X_valid, y_train, y_valid, params)\n",
    "\n",
    "print(\"\\nTest results ...\" )\n",
    "pred_test = predict_test(model_summary, df_test, 'summary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['pred_summary'] = pred_train\n",
    "df_valid['pred_summary'] = pred_valid\n",
    "df_test['pred_summary'] = pred_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomized search...\n",
      "Fitting 4 folds for each of 5 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:  1.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=<generator object _BaseKFold.split at 0x11d5a0f68>,\n",
       "          error_score='raise',\n",
       "          estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.001, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=10000,\n",
       "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1),\n",
       "          fit_params=None, iid=True, n_iter=5, n_jobs=-1,\n",
       "          param_distributions={'min_child_weight': [1, 3, 5], 'gamma': [0.5, 1, 1.5, 2], 'subsample': [0.6, 0.8, 1.0], 'colsample_bytree': [0.6, 0.8, 1.0], 'max_depth': [3, 4, 5], 'learning_rate': [0.1, 0.01, 0.005]},\n",
       "          pre_dispatch='2*n_jobs', random_state=1001, refit=True,\n",
       "          return_train_score='warn', scoring='roc_auc', verbose=1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "\n",
    "params = {\n",
    "        'min_child_weight': [1, 3, 5],\n",
    "        'gamma': [0.5, 1, 1.5, 2],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'max_depth': [3, 4, 5],\n",
    "        'learning_rate': [0.1, 0.01, 0.005]\n",
    "        }\n",
    "\n",
    "xgb = XGBClassifier(learning_rate=0.001, n_estimators=10000,\n",
    "                    objective='binary:logistic', silent=True)\n",
    "folds = 4\n",
    "param_comb = 5\n",
    "skf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1001)\n",
    "\n",
    "predictors = ['pred_text', 'pred_title', 'pred_summary']\n",
    "X_train = df_train[predictors].values\n",
    "y_train = df_train[flag].values.flatten()\n",
    "\n",
    "print(\"Randomized search...\")\n",
    "random_search = RandomizedSearchCV(xgb,\n",
    "                                   param_distributions=params,\n",
    "                                   n_iter=param_comb,\n",
    "                                   scoring='roc_auc',\n",
    "                                   n_jobs=-1,\n",
    "                                   cv=skf.split(X_train, y_train),\n",
    "                                   verbose=1,  # 2\n",
    "                                   random_state=1001 )\n",
    "random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = df_test[predictors].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = random_search.predict_proba(X_train)[:,1]\n",
    "pred_test = random_search.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score (train): 0.994422\n"
     ]
    }
   ],
   "source": [
    "print( \"AUC Score (train): %f\" % metrics.roc_auc_score(y_train, pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(random_search.best_estimator_, open('xgboost_random_search.dat', \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search2 = pickle.load(open('xgboost_random_search.dat', \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train2 = random_search2.predict_proba(X_train)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score (train): 0.994422\n"
     ]
    }
   ],
   "source": [
    "print( \"AUC Score (train): %f\" % metrics.roc_auc_score(y_train, pred_train2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_evaluate_xgboost(alg, \n",
    "             dtrain, \n",
    "             dtest, \n",
    "             predictors, \n",
    "             verbose=0, \n",
    "             useTrainCV=True, \n",
    "             cv_folds=5, \n",
    "             early_stopping_rounds=50, \n",
    "             flag='flag'):\n",
    "    \n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(dtrain[predictors].values, label=dtrain[flag].values.flatten())\n",
    "        cvresult = xgb.cv(\n",
    "            xgb_param, \n",
    "            xgtrain, \n",
    "            num_boost_round=alg.get_params()['n_estimators'], \n",
    "            nfold=cv_folds,\n",
    "            metrics='auc', \n",
    "            early_stopping_rounds=early_stopping_rounds, \n",
    "            verbose_eval=verbose)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "        print(alg.get_params())\n",
    "    \n",
    "    # Fit the algorithm on the data\n",
    "    alg.fit(dtrain[predictors], dtrain[flag].values.flatten(),eval_metric='auc')\n",
    "        \n",
    "    # Predict training set:\n",
    "    dtrain_predictions = alg.predict(dtrain[predictors])\n",
    "    dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n",
    "        \n",
    "    # Print model report:\n",
    "    print( \"\\nModel Report (Train)\")\n",
    "    print( \"Accuracy : %.4g\" % metrics.accuracy_score(dtrain[flag].values, dtrain_predictions))\n",
    "    print( \"AUC Score: %f\" % metrics.roc_auc_score(dtrain[flag].values, dtrain_predprob))\n",
    "    \n",
    "    # Predict validation set:\n",
    "    dtest_predprob = alg.predict_proba(dtest[predictors])[:,1]\n",
    "\n",
    "    # Print model report:\n",
    "    print( \"\\nModel Report (Test)\")\n",
    "    print( \"AUC Score: %f\" % metrics.roc_auc_score(dtest[flag].values, dtest_predprob))\n",
    "    \n",
    "    return alg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1, 'colsample_bytree': 0.8, 'gamma': 0, 'learning_rate': 0.01, 'max_delta_step': 0, 'max_depth': 5, 'min_child_weight': 1, 'missing': None, 'n_estimators': 187, 'n_jobs': 1, 'nthread': 4, 'objective': 'binary:logistic', 'random_state': 0, 'reg_alpha': 1, 'reg_lambda': 1, 'scale_pos_weight': 1, 'seed': 27, 'silent': True, 'subsample': 0.8}\n",
      "\n",
      "Model Report (Train)\n",
      "Accuracy : 0.9675\n",
      "AUC Score: 0.993087\n",
      "\n",
      "Model Report (Test)\n",
      "AUC Score: 0.939585\n"
     ]
    }
   ],
   "source": [
    "predictors = ['pred_text', 'pred_title', 'pred_summary']\n",
    "\n",
    "# xgb sparse matrix\n",
    "xgtrain = xgb.DMatrix(X_train, label= y_train)\n",
    "xgvalid = xgb.DMatrix(X_valid, label= y_valid)\n",
    "\n",
    "model_xgb = XGBClassifier(\n",
    " booster = 'gbtree',\n",
    " learning_rate =0.01,\n",
    " n_estimators=1000,\n",
    " max_depth=5,\n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " objective= 'binary:logistic',\n",
    " nthread=4,\n",
    " reg_alpha=1,\n",
    " scale_pos_weight=1,\n",
    " seed=27)\n",
    "\n",
    "model_xgb = fit_evaluate_xgboost(model_xgb, df_train, df_valid, predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving model_text ...\n",
      "\n",
      "Saving model_title ...\n",
      "\n",
      "Saving model_summary ...\n"
     ]
    }
   ],
   "source": [
    "def save_model(model, model_name):\n",
    "    print(\"\\nSaving\", model_name,\"...\")\n",
    "    model.save(\"./models/%s.h5\" % model_name)\n",
    "    \n",
    "save_model(model_text, \"model_text\")\n",
    "save_model(model_title, \"model_title\")\n",
    "save_model(model_summary, \"model_summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model to file\n",
    "pickle.dump(model_xgb, open(\"./models/model_xgb.pickle.dat\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model_summary.save('./models/model_summary.h5')\n",
    "model = load_model('./models/model_summary.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8982512372064424"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.roc_auc_score(y_valid, model.predict_proba(X_valid)[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Converting words to integers and prepadding ...\n",
      "The number of texts greater than 500 in length is:  5,372\n",
      "The number of texts less than 50 in length is:  130\n",
      "\n",
      "Converting words to integers and prepadding ...\n",
      "The number of texts greater than 500 in length is:  1,336\n",
      "The number of texts less than 50 in length is:  33\n"
     ]
    }
   ],
   "source": [
    "word_threshold = 500\n",
    "X_train = np.array(word2int(df_train, n_u_words, 'text', word_threshold))\n",
    "X_valid = np.array(word2int(df_valid, n_u_words, 'text', word_threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11709, 500)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_instance = X_valid[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 11709)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.transpose(X_train).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_txt = pd.DataFrame(X_train)\n",
    "df_valid_txt = pd.DataFrame(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/moyandreu/coding/bcpnews/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "import lime\n",
    "import lime.lime_tabular\n",
    "\n",
    "# create the lime explainer\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(df_train_txt.as_matrix(), feature_names=df_train_txt.columns) # X_train.values, , class_names=(0,1)\n",
    "\n",
    "predict_fn = lambda x: model_text.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/moyandreu/coding/bcpnews/lib/python3.6/site-packages/numpy/core/fromnumeric.py:52: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  return getattr(obj, method)(*args, **kwds)\n",
      "/Users/moyandreu/coding/bcpnews/lib/python3.6/site-packages/lime/lime_tabular.py:296: UserWarning: \n",
      "                    Prediction probabilties do not sum to 1, and\n",
      "                    thus does not constitute a probability space.\n",
      "                    Check that you classifier outputs probabilities\n",
      "                    (Not log probabilities, or actual class predictions).\n",
      "                    \n",
      "  \"\"\")\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 1 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-120-174f23107e55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mexp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexplainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplain_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_valid_txt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/coding/bcpnews/lib/python3.6/site-packages/lime/lime_tabular.py\u001b[0m in \u001b[0;36mexplain_instance\u001b[0;34m(self, data_row, predict_fn, labels, top_labels, num_features, num_samples, distance_metric, model_regressor)\u001b[0m\n\u001b[1;32m    370\u001b[0m                     \u001b[0mnum_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m                     \u001b[0mmodel_regressor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_regressor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m                     feature_selection=self.feature_selection)\n\u001b[0m\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"regression\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/coding/bcpnews/lib/python3.6/site-packages/lime/lime_base.py\u001b[0m in \u001b[0;36mexplain_instance_with_data\u001b[0;34m(self, neighborhood_data, neighborhood_labels, distances, label, num_features, feature_selection, model_regressor)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0mlabels_column\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneighborhood_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m         used_features = self.feature_selection(neighborhood_data,\n\u001b[1;32m    154\u001b[0m                                                \u001b[0mlabels_column\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for axis 1 with size 1"
     ]
    }
   ],
   "source": [
    "exp = explainer.explain_instance(np.reshape(df_valid_txt.loc[1], 500), predict_fn, num_features=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lime\n",
    "# ------------------------------------------------------------------------------\n",
    "print('\\nUsing Lime to explain instances...')\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "import re\n",
    "\n",
    "# create the lime explainer\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(df_train[features].as_matrix(), feature_names=features) # X_train.values, , class_names=(0,1)\n",
    "\n",
    "def lime_explain_instance(id):\n",
    "\n",
    "    test_instance_tot = test.loc[test[col_id]==id].head(1)\n",
    "    test_instance = test_instance_tot[features]\n",
    "    test_instance = test_instance.clip(-10000000.0, 10000000.0) # convert int to float instead?\n",
    "    test_instance = test_instance.values[0]\n",
    "\n",
    "    # prediction function: for classifiers, this should be function that takes a numpy array and outputs probability predictions\n",
    "    predict_fn_xgb = lambda x: clf.predict_proba(x).astype(float)\n",
    "\n",
    "    exp = explainer.explain_instance(test_instance, predict_fn_xgb, num_features=200) # test_instance.values\n",
    "    print('Document id     : %d' % (id))\n",
    "    print('Probability (=1):', clf.predict_proba([test_instance])[0,1])\n",
    "    print('True class      : %s' % test_instance_tot[col_target].values[0])\n",
    "\n",
    "    ll = []\n",
    "    for i in range(1, len(exp.as_list()), 1):\n",
    "        id_var = exp.as_map()[1][i][0]\n",
    "        var = features[id_var]\n",
    "        value = test_instance[id_var]\n",
    "        crit = exp.as_list()[i][0]\n",
    "        w = exp.as_list()[i][1]\n",
    "        dd = {\n",
    "            \"variable\": var,\n",
    "            \"value\": value,\n",
    "            \"explanation\": w,\n",
    "            \"criteria\": crit\n",
    "        }\n",
    "        ll.append(dd)\n",
    "\n",
    "    explainer_df = pd.DataFrame(ll)\n",
    "    explainer_df = explainer_df.sort_values('explanation', ascending=False)\n",
    "    explainer_df.head(10)\n",
    "    explainer_df.tail(10)\n",
    "\n",
    "    pyplot.bar(range(len(explainer_df)), explainer_df['explanation'].values)\n",
    "    ind = np.arange(len(explainer_df['variable'].values))    # the x locations for the groups\n",
    "    pyplot.xticks(ind, explainer_df['variable'].values, rotation='vertical')\n",
    "    # pyplot.savefig('3_gbm_raw_feature_importance.png', bbox_inches='tight')\n",
    "    pyplot.show()\n",
    "\n",
    "    return explainer_df\n",
    "\n",
    "# check top 15 of largest estimated probabilities\n",
    "test[['id', 'TARGET', 'predprob']].sort_values('predprob', ascending=False).head(15)\n",
    "\n",
    "\"\"\"\n",
    "2016030520890380\n",
    "2014120519399710\n",
    "2015120012335320\n",
    "2015060519288510\n",
    "2015090014583910\n",
    "2014120013445730\n",
    "\"\"\"\n",
    "explainer_df = lime_explain_instance(2016030520890380)\n",
    "\n",
    "explainer_df.head(10)\n",
    "explainer_df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_submission = df_test[['id', 'pred']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission_time = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print submission_time\n",
    "df_submission.to_csv('../submissions/submission_%s.csv' % submission_time, sep=\",\", na_rep=\"\", mode=\"w\", index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
