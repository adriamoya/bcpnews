{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amoya/coding/bcpnews/lib/python3.4/importlib/_bootstrap.py:321: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: iso-8859-15 -*-\n",
    "\n",
    "# RRN to classify text\n",
    "# Author: adriamoya\n",
    "\n",
    "#%matplotlib inline\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import random as rn\n",
    "import tensorflow as tf\n",
    "\n",
    "# The below is necessary for starting Numpy generated random numbers\n",
    "# in a well-defined initial state.\n",
    "np.random.seed(1337)\n",
    "\n",
    "# The below is necessary for starting core Python generated random numbers\n",
    "# in a well-defined state.\n",
    "rn.seed(12345)\n",
    "\n",
    "# Force TensorFlow to use single thread.\n",
    "# Multiple threads are a potential source of\n",
    "# non-reproducible results.\n",
    "# For further details, see: https://stackoverflow.com/questions/42022950/which-seeds-have-to-be-set-where-to-realize-100-reproducibility-of-training-res\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "\n",
    "from keras import backend as K\n",
    "# The below tf.set_random_seed() will make random number generation\n",
    "# in the TensorFlow backend have a well-defined initial state.\n",
    "# For further details, see: https://www.tensorflow.org/api_docs/python/tf/set_random_seed\n",
    "\n",
    "tf.set_random_seed(1234)\n",
    "\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)\n",
    "\n",
    "# Rest of code follows ...\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, BatchNormalization, LSTM, Embedding, Reshape, Conv1D, MaxPooling1D\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "#import xgboost as xgb\n",
    "#from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# variables\n",
    "flag = 'flag'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "df = pd.read_csv(\"../../1_construction/3_newspaper_scraper/analyses/cleaned_datasets/train.csv\")\n",
    "df_test  = pd.read_csv(\"../../1_construction/3_newspaper_scraper/analyses/cleaned_datasets/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ip calculation\n",
    "def ip(y_target, y_pred):\n",
    "    return 100*(2*(metrics.roc_auc_score(y_target, y_pred))-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocessing(df, column=\"text\"):\n",
    "    \"\"\"Preprocessing (lower case, remove urls, punctuations).\n",
    "    \n",
    "    Args:\n",
    "        df     : Dataset with articles information (pandas.DataFrame).\n",
    "        column : Name of the column that contains the text of the article. Default is `text`.\n",
    "        \n",
    "    Returns:\n",
    "        df     : Dataset with articles information (pandas.DataFrame).\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\nPreprocessing %s ...\" % (column))\n",
    "\n",
    "    # preprocessing steps: lower case, remove urls, punctuations ...\n",
    "    df[column] = df[column].str.lower()\n",
    "    df[column] = df[column].str.replace(r'http[\\w:/\\.]+','') # remove urls\n",
    "    df[column] = df[column].str.replace(r'[^\\.(a-zA-ZÀ-ÿ0-9)\\s]','') #remove everything but characters and punctuation ( [^\\.\\w\\s] )\n",
    "    df[column] = df[column].str.replace(r'(?<=\\d)(\\.)(?=\\d)','') #remove dots in thousands (careful with decimals!)\n",
    "    df[column] = df[column].str.replace(r'\\.\\.+','.') #replace multple periods with a single one\n",
    "    df[column] = df[column].str.replace(r'\\.',' .') #replace multple periods with a single one\n",
    "    df[column] = df[column].str.replace(r'\\(',' ') # replace brackets with white spaces\n",
    "    df[column] = df[column].str.replace(r'\\)',' ') # replace brackets with white spaces\n",
    "    df[column] = df[column].str.replace(r'\\s\\s+',' ') #replace multple white space with a single one\n",
    "    df[column] = df[column].str.strip()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dictionary(df, min_count_word=5):\n",
    "    \"\"\"Build dictionary and relationships between words and integers.\n",
    "    \n",
    "    Args:\n",
    "        df             : Dataset with articles information (pandas.DataFrame).\n",
    "        min_count_word : Only consider words that have been used more than n times. Default is 5.\n",
    "        \n",
    "    Returns:\n",
    "        word2num       : Dictionary (words to numbers).\n",
    "        num2word       : Dictionary (numbers to words).\n",
    "        n_u_words      : Length of the dictionary (number of unique words).\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\nBuilding dictionary ...\" )\n",
    "\n",
    "    # get all unique words (only consider words that have been used more than 5 times)\n",
    "    all_text = ' '.join(df.text.values)\n",
    "    words = all_text.split()\n",
    "    u_words = Counter(words).most_common()\n",
    "    u_words = [word[0] for word in u_words if word[1]>min_count_word] # we will only consider words that have been used more than 5 times\n",
    "\n",
    "    print('The number of unique words is:', \"{:,}\".format(len(u_words)))\n",
    "\n",
    "    # create the dictionary\n",
    "    word2num = dict(zip(u_words,range(len(u_words))))\n",
    "    word2num['<Other>'] = len(u_words)\n",
    "    num2word = dict(zip(word2num.values(), word2num.keys()))\n",
    "\n",
    "    num2word[len(word2num)] = '<PAD>'\n",
    "    word2num['<PAD>'] = len(word2num)\n",
    "    \n",
    "    n_u_words = len(u_words)\n",
    "\n",
    "    return word2num, num2word, n_u_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word2int(df, n_u_words, column='text', word_threshold=500):\n",
    "    \"\"\"Convert words to integers and prepad sentences\n",
    "    \n",
    "    Args:\n",
    "        df             : Dataset with articles information (pandas.DataFrame)\n",
    "        n_u_words      : Length of the dictionary (number of unique words).\n",
    "        column         : Name of the column that contains the text of the article. Default is `text`.\n",
    "        word_threshold : Number of words to consider for each text (padding). Default is 500.\n",
    "        \n",
    "    Returns:\n",
    "        int_text       : Array with texts translated to integers.\n",
    "        \"\"\"\n",
    "\n",
    "    print(\"\\nConverting words to integers and prepadding ...\" )\n",
    "\n",
    "    int_text = [[word2num[word] if word in word2num else n_u_words for word in Text.split()] for Text in df[column].values] # Text.split() python2\n",
    "\n",
    "    print('The number of texts greater than %s in length is: ' % str(word_threshold), \"{:,}\".format(np.sum(np.array([len(t)>word_threshold for t in int_text]))))\n",
    "    print('The number of texts less than 50 in length is: ', \"{:,}\".format(np.sum(np.array([len(t)<50 for t in int_text]))))\n",
    "\n",
    "    for i, t in enumerate(int_text):\n",
    "        if len(t)<word_threshold:\n",
    "            int_text[i] = [word2num['<PAD>']]*(word_threshold-len(t)) + t\n",
    "        elif len(t)>word_threshold:\n",
    "            int_text[i] = t[:word_threshold]\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    return int_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_test(model, X_test, column):\n",
    "    \"\"\"Make predictions in test dataset.\n",
    "    \n",
    "    Args:\n",
    "        model     : Model trained.\n",
    "        X_test    : Array with test features.\n",
    "        column    : Name of the column that contains desired feature.\n",
    "        \n",
    "    Returns:\n",
    "        pred_test : Array with test predictions.\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    # words to numbers\n",
    "    int_text = word2int(X_test, n_u_words, column, word_threshold)\n",
    "\n",
    "    X = np.array(int_text)\n",
    "\n",
    "    pred = model.predict(X)\n",
    "\n",
    "    l_pred = []\n",
    "    for item in pred:\n",
    "        l_pred.append(item[0])\n",
    "        \n",
    "    return l_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessing text ...\n",
      "\n",
      "Preprocessing text ...\n",
      "\n",
      "Preprocessing title ...\n",
      "\n",
      "Preprocessing title ...\n"
     ]
    }
   ],
   "source": [
    "# preprocessing steps: lower case, remove urls, punctuations ...\n",
    "\n",
    "# text\n",
    "df = preprocessing(df, 'text')\n",
    "df_test = preprocessing(df_test, 'text')\n",
    "\n",
    "# title\n",
    "df = preprocessing(df, 'title')\n",
    "df_test = preprocessing(df_test, 'title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building dictionary ...\n",
      "The number of unique words is: 41,112\n"
     ]
    }
   ],
   "source": [
    "# build dictionary\n",
    "min_count_word = 4\n",
    "word2num, num2word, n_u_words = build_dictionary(df, min_count_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train / Validation split ...\n",
      "X_train: (11709, 6)\n",
      "X_valid: (2928, 6)\n",
      "y_train: (11709,)\n",
      "y_valid: (2928,)\n"
     ]
    }
   ],
   "source": [
    "# train / validation split\n",
    "print(\"\\nTrain / Validation split ...\")\n",
    "\n",
    "X, y = df[df.columns[~df.columns.str.contains(flag)]].values, df[flag].values\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, stratify=y, shuffle=True, random_state=42)\n",
    "\n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"X_valid:\", X_valid.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"y_valid:\", y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns = df.columns[~df.columns.str.contains(flag)].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(X_train, columns=columns); df_train[flag] = y_train\n",
    "df_valid = pd.DataFrame(X_valid, columns=columns); df_valid[flag] = y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (11709, 7)\n",
      "Valid: (2928, 7)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train:\", df_train.shape)\n",
    "print(\"Valid:\", df_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_evaluate_model(X_train, X_valid, y_train, y_valid, params):\n",
    "    \"\"\"Fit and evaluate Many to One RNN\n",
    "    \n",
    "    Args:\n",
    "        X_train    : Array with train features.\n",
    "        X_valid    : Array with validation features.\n",
    "        y_train    : Array with train flag.\n",
    "        y_valid    : Array with validation flag.\n",
    "        params     : Dictionary with parameter configuration.\n",
    "        \n",
    "    Returns:\n",
    "        model      : Model already trained.\n",
    "        pred_train : Array with train predictions.\n",
    "        pred_valid : Array with validation predictions.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\nCreating Sequential RNN: Many to One...\" )\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='loss', patience=2)\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Embedding(len(word2num), params['embedding_size'])) # , batch_size=batch_size\n",
    "    model.add(Conv1D(filters=128, kernel_size=5, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Conv1D(filters=128, kernel_size=5, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(LSTM(100))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='sigmoid')) # sigmoid\n",
    "    \n",
    "    model.compile(loss=params['loss_func'], optimizer=params['optimizer'], metrics=params['metrics'])\n",
    "    model.summary()\n",
    "\n",
    "    batch_size = params['batch_size']\n",
    "    print(\"\\nFitting the model ...\" )\n",
    "    model.fit(X_train, y_train, batch_size=batch_size, epochs=params['epochs'], callbacks=[early_stopping])\n",
    "    \n",
    "    print(\"\\nPredicting probs on train ...\" )\n",
    "    pred_train = model.predict(X_train)\n",
    "    print(\"\\nAUC: {0:.2f}%\".format(100*metrics.roc_auc_score(y_train, pred_train)), \"| GINI: {0:.2f}%\".format(ip(y_train, pred_train)))\n",
    "\n",
    "    print(\"\\nEvaluating in valid ...\" )\n",
    "    print(model.evaluate(X_valid, y_valid, batch_size=batch_size))\n",
    "    \n",
    "    print(\"\\nPredicting probs on valid ...\" )\n",
    "    pred_valid = model.predict(X_valid)\n",
    "    print(\"\\nAUC: {0:.2f}%\".format(100*metrics.roc_auc_score(y_valid, pred_valid)), \"| GINI: {0:.2f}%\".format(ip(y_valid, pred_valid)))\n",
    "\n",
    "    return model, pred_train, pred_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train\n",
      "\n",
      "Converting words to integers and prepadding ...\n",
      "The number of texts greater than 500 in length is:  5,372\n",
      "The number of texts less than 50 in length is:  130\n",
      "\n",
      "Valid\n",
      "\n",
      "Converting words to integers and prepadding ...\n",
      "The number of texts greater than 500 in length is:  1,336\n",
      "The number of texts less than 50 in length is:  33\n"
     ]
    }
   ],
   "source": [
    "word_threshold = 500\n",
    "\n",
    "params = {\n",
    "    'loss_func': 'binary_crossentropy', # binary_crossentropy\n",
    "    'optimizer': 'rmsprop', # adam, rmsprop\n",
    "    'metrics': ['accuracy'],\n",
    "    'embedding_size': 100,\n",
    "    'batch_size': 128,\n",
    "    'epochs': 3\n",
    "}\n",
    "\n",
    "# word to integers\n",
    "print(\"\\nTrain\")\n",
    "X_train = np.array(word2int(df_train, n_u_words, 'text', word_threshold))\n",
    "print(\"\\nValid\")\n",
    "X_valid = np.array(word2int(df_valid, n_u_words, 'text', word_threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating Sequential RNN: Many to One...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 100)         4111400   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, None, 128)         64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, None, 128)         82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               91600     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 4,349,277\n",
      "Trainable params: 4,349,277\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Fitting the model ...\n",
      "Epoch 1/3\n",
      "11709/11709 [==============================] - 261s 22ms/step - loss: 0.4310 - acc: 0.8083\n",
      "Epoch 2/3\n",
      "11709/11709 [==============================] - 254s 22ms/step - loss: 0.2507 - acc: 0.9002\n",
      "Epoch 3/3\n",
      "11709/11709 [==============================] - 253s 22ms/step - loss: 0.1884 - acc: 0.9307\n",
      "\n",
      "Predicting probs on train ...\n",
      "\n",
      "AUC: 98.77% | GINI: 97.53%\n",
      "\n",
      "Evaluating in valid ...\n",
      "2928/2928 [==============================] - 25s 8ms/step\n",
      "[0.33804667533421123, 0.8657786895017154]\n",
      "\n",
      "Predicting probs on valid ...\n",
      "\n",
      "AUC: 92.32% | GINI: 84.65%\n",
      "\n",
      "Test results ...\n",
      "\n",
      "Converting words to integers and prepadding ...\n",
      "The number of texts greater than 500 in length is:  2,654\n",
      "The number of texts less than 50 in length is:  45\n"
     ]
    }
   ],
   "source": [
    "model_text, pred_train, pred_valid = fit_evaluate_model(X_train, X_valid, y_train, y_valid, params)\n",
    "\n",
    "print(\"\\nTest results ...\" )\n",
    "pred_test = predict_test(model_text, df_test, 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train['pred_text'] = pred_train\n",
    "df_valid['pred_text'] = pred_valid\n",
    "df_test['pred_text'] = pred_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Converting words to integers and prepadding ...\n",
      "The number of texts greater than 15 in length is:  1,957\n",
      "The number of texts less than 50 in length is:  11,709\n",
      "\n",
      "Converting words to integers and prepadding ...\n",
      "The number of texts greater than 15 in length is:  466\n",
      "The number of texts less than 50 in length is:  2,928\n"
     ]
    }
   ],
   "source": [
    "word_threshold = 15\n",
    "\n",
    "params = {\n",
    "    'loss_func': 'binary_crossentropy', # binary_crossentropy\n",
    "    'optimizer': 'rmsprop', # adam, rmsprop\n",
    "    'metrics': ['accuracy'],\n",
    "    'embedding_size': 100,\n",
    "    'batch_size': 128,\n",
    "    'epochs': 3\n",
    "}\n",
    "\n",
    "# word to integer\n",
    "X_train = np.array(word2int(df_train, n_u_words, 'title', word_threshold))\n",
    "X_valid = np.array(word2int(df_valid, n_u_words, 'title', word_threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating Sequential RNN: Many to One...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 100)         4111400   \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, None, 128)         64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, None, 128)         82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               91600     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 4,349,277\n",
      "Trainable params: 4,349,277\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Fitting the model ...\n",
      "Epoch 1/3\n",
      "11709/11709 [==============================] - 33s 3ms/step - loss: 0.3993 - acc: 0.8253\n",
      "Epoch 2/3\n",
      "11709/11709 [==============================] - 32s 3ms/step - loss: 0.2271 - acc: 0.9109\n",
      "Epoch 3/3\n",
      "11709/11709 [==============================] - 28s 2ms/step - loss: 0.1763 - acc: 0.9325\n",
      "\n",
      "Predicting probs on train ...\n",
      "\n",
      "AUC: 98.58% | GINI: 97.17%\n",
      "\n",
      "Evaluating in valid ...\n",
      "2928/2928 [==============================] - 2s 564us/step\n",
      "[0.29333466519423523, 0.8835382516918286]\n",
      "\n",
      "Predicting probs on valid ...\n",
      "\n",
      "AUC: 93.68% | GINI: 87.36%\n",
      "\n",
      "Test results ...\n",
      "\n",
      "Converting words to integers and prepadding ...\n",
      "The number of texts greater than 15 in length is:  1,373\n",
      "The number of texts less than 50 in length is:  5,225\n"
     ]
    }
   ],
   "source": [
    "model_title, pred_train, pred_valid = fit_evaluate_model(X_train, X_valid, y_train, y_valid, params)\n",
    "\n",
    "print(\"\\nTest results ...\" )\n",
    "pred_test = predict_test(model_title, df_test, 'title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train['pred_title'] = pred_train\n",
    "df_valid['pred_title'] = pred_valid\n",
    "df_test['pred_title'] = pred_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Converting words to integers and prepadding ...\n",
      "The number of texts greater than 250 in length is:  39\n",
      "The number of texts less than 50 in length is:  160\n",
      "\n",
      "Converting words to integers and prepadding ...\n",
      "The number of texts greater than 250 in length is:  8\n",
      "The number of texts less than 50 in length is:  35\n"
     ]
    }
   ],
   "source": [
    "word_threshold = 250\n",
    "\n",
    "params = {\n",
    "    'loss_func': 'binary_crossentropy', # binary_crossentropy\n",
    "    'optimizer': 'rmsprop', # adam, rmsprop\n",
    "    'metrics': ['accuracy'],\n",
    "    'embedding_size': 100,\n",
    "    'batch_size': 128,\n",
    "    'epochs': 3\n",
    "}\n",
    "\n",
    "# word to integer\n",
    "X_train = np.array(word2int(df_train, n_u_words, 'summary', word_threshold))\n",
    "X_valid = np.array(word2int(df_valid, n_u_words, 'summary', word_threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating Sequential RNN: Many to One...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, None, 100)         4111400   \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, None, 128)         64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, None, 128)         82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               91600     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 4,349,277\n",
      "Trainable params: 4,349,277\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Fitting the model ...\n",
      "Epoch 1/3\n",
      "11709/11709 [==============================] - 126s 11ms/step - loss: 0.5073 - acc: 0.7690\n",
      "Epoch 2/3\n",
      "11709/11709 [==============================] - 124s 11ms/step - loss: 0.2999 - acc: 0.8770\n",
      "Epoch 3/3\n",
      "11709/11709 [==============================] - 136s 12ms/step - loss: 0.2353 - acc: 0.9066\n",
      "\n",
      "Predicting probs on train ...\n",
      "\n",
      "AUC: 97.87% | GINI: 95.74%\n",
      "\n",
      "Evaluating in valid ...\n",
      "2928/2928 [==============================] - 12s 4ms/step\n",
      "[0.39172577874256614, 0.8514344259037998]\n",
      "\n",
      "Predicting probs on valid ...\n",
      "\n",
      "AUC: 89.73% | GINI: 79.46%\n",
      "\n",
      "Test results ...\n",
      "\n",
      "Converting words to integers and prepadding ...\n",
      "The number of texts greater than 250 in length is:  28\n",
      "The number of texts less than 50 in length is:  56\n"
     ]
    }
   ],
   "source": [
    "model_summary, pred_train, pred_valid = fit_evaluate_model(X_train, X_valid, y_train, y_valid, params)\n",
    "\n",
    "print(\"\\nTest results ...\" )\n",
    "pred_test = predict_test(model_summary, df_test, 'summary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train['pred_summary'] = pred_train\n",
    "df_valid['pred_summary'] = pred_valid\n",
    "df_test['pred_summary'] = pred_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "/lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.23' not found (required by /home/amoya/coding/bcpnews/lib/python3.4/site-packages/xgboost/./lib/libxgboost.so)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-d50d4e478382>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m def model_xgboost(alg, \n\u001b[1;32m      5\u001b[0m              \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/coding/bcpnews/lib/python3.4/site-packages/xgboost/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDMatrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBooster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrabit\u001b[0m                   \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/coding/bcpnews/lib/python3.4/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;31m# load the XGBoost library globally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m \u001b[0m_LIB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_lib\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/coding/bcpnews/lib/python3.4/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_load_lib\u001b[0;34m()\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlib_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m     \u001b[0mlib\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcdll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoadLibrary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlib_path\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m     \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBGetLastError\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_char_p\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.4/ctypes/__init__.py\u001b[0m in \u001b[0;36mLoadLibrary\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mLoadLibrary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dlltype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[0mcdll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLibraryLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCDLL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.4/ctypes/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_dlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: /lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.23' not found (required by /home/amoya/coding/bcpnews/lib/python3.4/site-packages/xgboost/./lib/libxgboost.so)"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "def model_xgboost(alg, \n",
    "             dtrain, \n",
    "             dtest, \n",
    "             predictors, \n",
    "             verbose=0, \n",
    "             useTrainCV=True, \n",
    "             cv_folds=5, \n",
    "             early_stopping_rounds=50, \n",
    "             flag='flag'):\n",
    "    \n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(dtrain[predictors].values, label=dtrain[flag].values.flatten())\n",
    "        cvresult = xgb.cv(\n",
    "            xgb_param, \n",
    "            xgtrain, \n",
    "            num_boost_round=alg.get_params()['n_estimators'], \n",
    "            nfold=cv_folds,\n",
    "            metrics='auc', \n",
    "            early_stopping_rounds=early_stopping_rounds, \n",
    "            verbose_eval=verbose)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "        print(alg.get_params())\n",
    "    \n",
    "    # Fit the algorithm on the data\n",
    "    alg.fit(dtrain[predictors], dtrain[flag].values.flatten(),eval_metric='auc')\n",
    "        \n",
    "    # Predict training set:\n",
    "    dtrain_predictions = alg.predict(dtrain[predictors])\n",
    "    dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n",
    "        \n",
    "    # Print model report:\n",
    "    print( \"\\nModel Report (Train)\")\n",
    "    print( \"Accuracy : %.4g\" % metrics.accuracy_score(dtrain[flag].values, dtrain_predictions))\n",
    "    print( \"AUC Score: %f\" % metrics.roc_auc_score(dtrain[flag].values, dtrain_predprob))\n",
    "    \n",
    "    # Predict validation set:\n",
    "    dtest_predprob = alg.predict_proba(dtest[predictors])[:,1]\n",
    "\n",
    "    # Print model report:\n",
    "    print( \"\\nModel Report (Test)\")\n",
    "    print( \"AUC Score: %f\" % metrics.roc_auc_score(dtest[flag].values, dtest_predprob))\n",
    "    \n",
    "    features_df = pd.DataFrame({'feature': pd.Series(predictors), 'importance': alg.feature_importances_})\n",
    "    features_df = features_df.sort_values('importance', ascending=False)\n",
    "    ind = np.arange(len(features_df['feature'].values))    # the x locations for the groups\n",
    "    \n",
    "    pyplot.figure(num=None, figsize=[12,4])\n",
    "    pyplot.bar(range(len(features_df)), features_df['importance'].values)\n",
    "    pyplot.xticks(ind, features_df['feature'].values, rotation='vertical')\n",
    "    pyplot.ylabel('Feature Importance Score')\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = df_train[['pred_text', 'pred_title', 'pred_summary']].values\n",
    "X_valid = df_valid[['pred_text', 'pred_title', 'pred_summary']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "/lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.23' not found (required by /home/amoya/coding/bcpnews/lib/python3.4/site-packages/xgboost/./lib/libxgboost.so)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-99b35e8cd67c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# xgb sparse matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mxgtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/coding/bcpnews/lib/python3.4/site-packages/xgboost/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDMatrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBooster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrabit\u001b[0m                   \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/coding/bcpnews/lib/python3.4/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;31m# load the XGBoost library globally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m \u001b[0m_LIB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_lib\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/coding/bcpnews/lib/python3.4/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_load_lib\u001b[0;34m()\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlib_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m     \u001b[0mlib\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcdll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoadLibrary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlib_path\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m     \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBGetLastError\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_char_p\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.4/ctypes/__init__.py\u001b[0m in \u001b[0;36mLoadLibrary\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mLoadLibrary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dlltype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[0mcdll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLibraryLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCDLL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.4/ctypes/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_dlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: /lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.23' not found (required by /home/amoya/coding/bcpnews/lib/python3.4/site-packages/xgboost/./lib/libxgboost.so)"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# xgb sparse matrix\n",
    "xgtrain = xgb.DMatrix(X_train, label= y_train)\n",
    "xgvalid = xgb.DMatrix(X_valid, label= y_valid)\n",
    "\n",
    "clf = XGBClassifier(\n",
    "    booster = 'gbtree',\n",
    "    learning_rate =0.1,\n",
    "    n_estimators=3000, #3000\n",
    "    max_depth=5,\n",
    "    min_child_weight=1,\n",
    "    gamma=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective= 'binary:logistic',\n",
    "    nthread=4,\n",
    "    scale_pos_weight=1,\n",
    "    seed=99)\n",
    "\n",
    "xgb_param = clf.get_xgb_params()\n",
    "\n",
    "# cross-validation\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "cv_folds = 5\n",
    "early_stopping_rounds = 50\n",
    "\n",
    "print('\\nInitializing cross-validation...')\n",
    "cvresult = xgb.cv(\n",
    "    xgb_param,\n",
    "    xgtrain,\n",
    "    num_boost_round=clf.get_params()['n_estimators'],\n",
    "    nfold=cv_folds,\n",
    "    metrics='auc',\n",
    "    early_stopping_rounds=early_stopping_rounds,\n",
    "    verbose_eval=1)\n",
    "\n",
    "# retrieve parameters\n",
    "print('\\nXGBClassifier parameters')\n",
    "clf.set_params(n_estimators=cvresult.shape[0])\n",
    "\n",
    "# fit the algorithm on the training data\n",
    "print('\\nFit algorithm on train data...')\n",
    "clf.fit(X_train, y_train, eval_metric='auc')\n",
    "\n",
    "# Predict training set\n",
    "# ------------------------------------------------------------------------------\n",
    "print('\\nPredicting on training set...' )\n",
    "dtrain_predictions = clf.predict(X_train)\n",
    "dtrain_predprob = clf.predict_proba(X_train)[:,1]\n",
    "\n",
    "# print model report:\n",
    "print('Model Report' )\n",
    "print('Accuracy : %.4g' % metrics.accuracy_score(y_train , dtrain_predictions) )\n",
    "print('AUC Score (Train): %f' % metrics.roc_auc_score(y_train, dtrain_predprob) )\n",
    "print('IP Score  (Train): %f' % ip(y_train, dtrain_predprob) )\n",
    "\n",
    "# Predict valid set\n",
    "# ------------------------------------------------------------------------------\n",
    "print('\\nPredicting on valid set...' )\n",
    "dvalid_predprob = clf.predict_proba(X_valid)[:,1]\n",
    "\n",
    "# print model report:\n",
    "print('Model Report' )\n",
    "print('AUC Score (Valid): %f' % metrics.roc_auc_score(y_valid, dvalid_predprob) )\n",
    "print('IP Score  (Valid): %f' % ip(y_valid, dvalid_predprob) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = np.array(df_test[['pred_text', 'pred_title']])\n",
    "\n",
    "# Predict test set\n",
    "# ------------------------------------------------------------------------------\n",
    "print('\\nPredicting on test set...' )\n",
    "dtest_predprob = clf.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test['pred'] = dtest_predprob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_threshold = 500\n",
    "X_train = np.array(word2int(df_train, n_u_words, 'text', word_threshold))\n",
    "X_valid = np.array(word2int(df_valid, n_u_words, 'text', word_threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_instance = X_valid[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.transpose(X_train).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train_txt = pd.DataFrame(X_train)\n",
    "df_valid_txt = pd.DataFrame(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lime\n",
    "import lime.lime_tabular\n",
    "\n",
    "# create the lime explainer\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(df_train_text.as_matrix(), feature_names=df_train_txt.columns) # X_train.values, , class_names=(0,1)\n",
    "\n",
    "predict_fn = lambda x: model_text.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exp = explainer.explain_instance(np.reshape(df_valid_txt.loc[1], 500), predict_fn, num_features=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lime\n",
    "# ------------------------------------------------------------------------------\n",
    "print('\\nUsing Lime to explain instances...')\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "import re\n",
    "\n",
    "# create the lime explainer\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(df_train[features].as_matrix(), feature_names=features) # X_train.values, , class_names=(0,1)\n",
    "\n",
    "def lime_explain_instance(id):\n",
    "\n",
    "    test_instance_tot = test.loc[test[col_id]==id].head(1)\n",
    "    test_instance = test_instance_tot[features]\n",
    "    test_instance = test_instance.clip(-10000000.0, 10000000.0) # convert int to float instead?\n",
    "    test_instance = test_instance.values[0]\n",
    "\n",
    "    # prediction function: for classifiers, this should be function that takes a numpy array and outputs probability predictions\n",
    "    predict_fn_xgb = lambda x: clf.predict_proba(x).astype(float)\n",
    "\n",
    "    exp = explainer.explain_instance(test_instance, predict_fn_xgb, num_features=200) # test_instance.values\n",
    "    print('Document id     : %d' % (id))\n",
    "    print('Probability (=1):', clf.predict_proba([test_instance])[0,1])\n",
    "    print('True class      : %s' % test_instance_tot[col_target].values[0])\n",
    "\n",
    "    ll = []\n",
    "    for i in range(1, len(exp.as_list()), 1):\n",
    "        id_var = exp.as_map()[1][i][0]\n",
    "        var = features[id_var]\n",
    "        value = test_instance[id_var]\n",
    "        crit = exp.as_list()[i][0]\n",
    "        w = exp.as_list()[i][1]\n",
    "        dd = {\n",
    "            \"variable\": var,\n",
    "            \"value\": value,\n",
    "            \"explanation\": w,\n",
    "            \"criteria\": crit\n",
    "        }\n",
    "        ll.append(dd)\n",
    "\n",
    "    explainer_df = pd.DataFrame(ll)\n",
    "    explainer_df = explainer_df.sort_values('explanation', ascending=False)\n",
    "    explainer_df.head(10)\n",
    "    explainer_df.tail(10)\n",
    "\n",
    "    pyplot.bar(range(len(explainer_df)), explainer_df['explanation'].values)\n",
    "    ind = np.arange(len(explainer_df['variable'].values))    # the x locations for the groups\n",
    "    pyplot.xticks(ind, explainer_df['variable'].values, rotation='vertical')\n",
    "    # pyplot.savefig('3_gbm_raw_feature_importance.png', bbox_inches='tight')\n",
    "    pyplot.show()\n",
    "\n",
    "    return explainer_df\n",
    "\n",
    "# check top 15 of largest estimated probabilities\n",
    "test[['id', 'TARGET', 'predprob']].sort_values('predprob', ascending=False).head(15)\n",
    "\n",
    "\"\"\"\n",
    "2016030520890380\n",
    "2014120519399710\n",
    "2015120012335320\n",
    "2015060519288510\n",
    "2015090014583910\n",
    "2014120013445730\n",
    "\"\"\"\n",
    "explainer_df = lime_explain_instance(2016030520890380)\n",
    "\n",
    "explainer_df.head(10)\n",
    "explainer_df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_submission = df_test[['id', 'pred']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission_time = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print submission_time\n",
    "df_submission.to_csv('../submissions/submission_%s.csv' % submission_time, sep=\",\", na_rep=\"\", mode=\"w\", index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
